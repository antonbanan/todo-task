* 
* ==> Audit <==
* |------------|--------------------------|----------|-------|---------|---------------------|---------------------|
|  Command   |           Args           | Profile  | User  | Version |     Start Time      |      End Time       |
|------------|--------------------------|----------|-------|---------|---------------------|---------------------|
| start      |                          | minikube | banan | v1.31.1 | 10 Aug 23 16:39 PDT | 10 Aug 23 16:44 PDT |
| stop       |                          | minikube | banan | v1.31.1 | 10 Aug 23 16:45 PDT | 10 Aug 23 16:45 PDT |
| start      |                          | minikube | banan | v1.31.1 | 11 Aug 23 08:53 PDT | 11 Aug 23 08:54 PDT |
| docker-env |                          | minikube | banan | v1.31.1 | 11 Aug 23 08:56 PDT | 11 Aug 23 08:56 PDT |
| docker-env | minikube docker-env      | minikube | banan | v1.31.1 | 11 Aug 23 08:57 PDT | 11 Aug 23 08:58 PDT |
| stop       |                          | minikube | banan | v1.31.1 | 11 Aug 23 10:24 PDT |                     |
| start      |                          | minikube | banan | v1.31.1 | 11 Aug 23 11:41 PDT | 11 Aug 23 11:42 PDT |
| docker-env |                          | minikube | banan | v1.31.1 | 11 Aug 23 11:48 PDT | 11 Aug 23 11:48 PDT |
| docker-env |                          | minikube | banan | v1.31.1 | 11 Aug 23 11:49 PDT | 11 Aug 23 11:49 PDT |
| docker-env | minikube docker-env      | minikube | banan | v1.31.1 | 11 Aug 23 11:49 PDT | 11 Aug 23 11:49 PDT |
| delete     |                          | minikube | banan | v1.31.1 | 11 Aug 23 12:01 PDT | 11 Aug 23 12:01 PDT |
| start      |                          | minikube | banan | v1.31.1 | 11 Aug 23 12:02 PDT | 11 Aug 23 12:03 PDT |
| docker-env |                          | minikube | banan | v1.31.1 | 11 Aug 23 12:04 PDT | 11 Aug 23 12:04 PDT |
| docker-env | minikube docker-env      | minikube | banan | v1.31.1 | 11 Aug 23 12:04 PDT | 11 Aug 23 12:04 PDT |
| service    | todo-front --url         | minikube | banan | v1.31.1 | 11 Aug 23 12:23 PDT |                     |
| service    | todo-front-service --url | minikube | banan | v1.31.1 | 11 Aug 23 12:23 PDT | 11 Aug 23 12:25 PDT |
| service    | todo-front-service --url | minikube | banan | v1.31.1 | 11 Aug 23 12:27 PDT |                     |
| service    | todo-app-service --url   | minikube | banan | v1.31.1 | 11 Aug 23 12:28 PDT | 11 Aug 23 12:29 PDT |
| docker-env | minikube docker-env      | minikube | banan | v1.31.1 | 11 Aug 23 16:02 PDT | 11 Aug 23 16:02 PDT |
| stop       |                          | minikube | banan | v1.31.1 | 11 Aug 23 16:36 PDT | 11 Aug 23 16:36 PDT |
| delete     | --all                    | minikube | banan | v1.31.1 | 11 Aug 23 16:37 PDT | 11 Aug 23 16:38 PDT |
| start      |                          | minikube | banan | v1.31.1 | 11 Aug 23 16:38 PDT | 11 Aug 23 16:39 PDT |
| docker-env |                          | minikube | banan | v1.31.1 | 11 Aug 23 16:39 PDT | 11 Aug 23 16:39 PDT |
| docker-env | minikube docker-env      | minikube | banan | v1.31.1 | 11 Aug 23 16:40 PDT | 11 Aug 23 16:40 PDT |
| service    | todo-app-service --url   | minikube | banan | v1.31.1 | 11 Aug 23 17:09 PDT | 11 Aug 23 17:16 PDT |
| service    | todo-app-service --url   | minikube | banan | v1.31.1 | 11 Aug 23 17:16 PDT | 11 Aug 23 17:16 PDT |
| service    | todo-app-service --url   | minikube | banan | v1.31.1 | 11 Aug 23 17:16 PDT |                     |
| start      |                          | minikube | banan | v1.31.1 | 12 Aug 23 12:37 PDT | 12 Aug 23 12:38 PDT |
| service    | todo-app --url           | minikube | banan | v1.31.1 | 12 Aug 23 12:57 PDT | 12 Aug 23 13:06 PDT |
| ip         |                          | minikube | banan | v1.31.1 | 12 Aug 23 13:05 PDT | 12 Aug 23 13:05 PDT |
| service    | todo-front-service --url | minikube | banan | v1.31.1 | 12 Aug 23 13:30 PDT |                     |
| service    | todo-front --url         | minikube | banan | v1.31.1 | 12 Aug 23 13:30 PDT |                     |
| service    | todo-front --url         | minikube | banan | v1.31.1 | 12 Aug 23 13:31 PDT |                     |
| delete     |                          | minikube | banan | v1.31.1 | 12 Aug 23 13:36 PDT | 12 Aug 23 13:36 PDT |
| start      |                          | minikube | banan | v1.31.1 | 12 Aug 23 13:37 PDT | 12 Aug 23 13:38 PDT |
| docker-env |                          | minikube | banan | v1.31.1 | 12 Aug 23 13:41 PDT | 12 Aug 23 13:41 PDT |
| docker-env | minikube docker-env      | minikube | banan | v1.31.1 | 12 Aug 23 13:42 PDT | 12 Aug 23 13:42 PDT |
| service    | todo-front --url         | minikube | banan | v1.31.1 | 12 Aug 23 13:46 PDT |                     |
| service    | todo-app --url           | minikube | banan | v1.31.1 | 12 Aug 23 13:46 PDT | 12 Aug 23 13:46 PDT |
| service    | todo-front --url         | minikube | banan | v1.31.1 | 12 Aug 23 13:47 PDT |                     |
| service    | todo-front --url         | minikube | banan | v1.31.1 | 12 Aug 23 13:52 PDT |                     |
| service    | todo --url               | minikube | banan | v1.31.1 | 12 Aug 23 13:57 PDT |                     |
| service    | todo-app --url           | minikube | banan | v1.31.1 | 12 Aug 23 13:57 PDT | 12 Aug 23 13:58 PDT |
| service    | todo-app --url           | minikube | banan | v1.31.1 | 12 Aug 23 13:58 PDT |                     |
| dashboard  |                          | minikube | banan | v1.31.1 | 12 Aug 23 14:00 PDT |                     |
| service    | todo                     | minikube | banan | v1.31.1 | 12 Aug 23 14:03 PDT |                     |
|------------|--------------------------|----------|-------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/08/12 13:37:41
Running on machine: MBP-Anton
Binary: Built with gc go1.20.6 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0812 13:37:41.785563   24335 out.go:296] Setting OutFile to fd 1 ...
I0812 13:37:41.787566   24335 out.go:348] isatty.IsTerminal(1) = true
I0812 13:37:41.787572   24335 out.go:309] Setting ErrFile to fd 2...
I0812 13:37:41.787577   24335 out.go:348] isatty.IsTerminal(2) = true
I0812 13:37:41.787872   24335 root.go:338] Updating PATH: /Users/banan/.minikube/bin
I0812 13:37:41.790847   24335 out.go:303] Setting JSON to false
I0812 13:37:41.838790   24335 start.go:128] hostinfo: {"hostname":"MBP-Anton.hitronhub.home","uptime":356506,"bootTime":1691516155,"procs":463,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"11.7.7","kernelVersion":"20.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"d1226be0-d446-53ef-bdc3-80600f23ac79"}
W0812 13:37:41.838915   24335 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0812 13:37:41.843680   24335 out.go:177] 😄  minikube v1.31.1 на Darwin 11.7.7
I0812 13:37:41.858166   24335 notify.go:220] Checking for updates...
I0812 13:37:41.859317   24335 driver.go:373] Setting default libvirt URI to qemu:///system
I0812 13:37:41.859771   24335 global.go:111] Querying for installed drivers using PATH=/Users/banan/.minikube/bin:/Users/banan/.nvm/versions/node/v18.16.1/bin:/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/banan/.nvm/versions/node/v14.21.3/bin
I0812 13:37:41.864118   24335 global.go:122] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0812 13:37:41.864209   24335 global.go:122] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0812 13:37:41.864420   24335 global.go:122] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0812 13:37:41.864925   24335 global.go:122] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0812 13:37:41.864932   24335 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0812 13:37:42.238336   24335 virtualbox.go:136] virtual box version: 7.0.10r158379
I0812 13:37:42.238378   24335 global.go:122] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:7.0.10r158379
}
I0812 13:37:42.238486   24335 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0812 13:37:42.471680   24335 docker.go:121] docker version: linux-24.0.2:Docker Desktop 4.21.1 (114176)
I0812 13:37:42.471907   24335 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0812 13:37:43.034328   24335 info.go:266] docker info: {ID:04150d1e-acf3-4da0-ba72-5eb68b0e2062 Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:false NGoroutines:75 SystemTime:2023-08-12 20:37:42.981228279 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:8 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8241242112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/banan/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.0] map[Name:compose Path:/Users/banan/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.19.1] map[Name:dev Path:/Users/banan/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/banan/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/banan/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/Users/banan/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/banan/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/banan/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.16.1]] Warnings:<nil>}}
I0812 13:37:43.034447   24335 global.go:122] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0812 13:37:43.034465   24335 driver.go:308] not recommending "ssh" due to default: false
I0812 13:37:43.034480   24335 driver.go:343] Picked: docker
I0812 13:37:43.034485   24335 driver.go:344] Alternatives: [virtualbox ssh]
I0812 13:37:43.034488   24335 driver.go:345] Rejects: [hyperkit parallels podman qemu2 vmware]
I0812 13:37:43.045848   24335 out.go:177] ✨  Automatically selected the docker driver. Other choices: virtualbox, ssh
I0812 13:37:43.053631   24335 start.go:298] selected driver: docker
I0812 13:37:43.053643   24335 start.go:898] validating driver "docker" against <nil>
I0812 13:37:43.053655   24335 start.go:909] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0812 13:37:43.053869   24335 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0812 13:37:43.272814   24335 info.go:266] docker info: {ID:04150d1e-acf3-4da0-ba72-5eb68b0e2062 Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:false NGoroutines:75 SystemTime:2023-08-12 20:37:43.198397076 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:8 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8241242112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/banan/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.0] map[Name:compose Path:/Users/banan/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.19.1] map[Name:dev Path:/Users/banan/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/banan/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/banan/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/Users/banan/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/banan/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/banan/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.16.1]] Warnings:<nil>}}
I0812 13:37:43.273047   24335 start_flags.go:305] no existing cluster config was found, will generate one from the flags 
I0812 13:37:43.273947   24335 start_flags.go:382] Using suggested 4000MB memory alloc based on sys=16384MB, container=7859MB
I0812 13:37:43.276422   24335 start_flags.go:901] Wait components to verify : map[apiserver:true system_pods:true]
I0812 13:37:43.280197   24335 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0812 13:37:43.288260   24335 cni.go:84] Creating CNI manager for ""
I0812 13:37:43.288294   24335 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0812 13:37:43.288320   24335 start_flags.go:314] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0812 13:37:43.288337   24335 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0812 13:37:43.295933   24335 out.go:177] 👍  Запускается control plane узел minikube в кластере minikube
I0812 13:37:43.311042   24335 cache.go:122] Beginning downloading kic base image for docker with docker
I0812 13:37:43.317568   24335 out.go:177] 🚜  Скачивается базовый образ ...
I0812 13:37:43.324869   24335 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0812 13:37:43.324993   24335 preload.go:148] Found local preload: /Users/banan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4
I0812 13:37:43.325029   24335 cache.go:57] Caching tarball of preloaded images
I0812 13:37:43.325095   24335 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0812 13:37:43.326226   24335 preload.go:174] Found /Users/banan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0812 13:37:43.326284   24335 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.3 on docker
I0812 13:37:43.327915   24335 profile.go:148] Saving config to /Users/banan/.minikube/profiles/minikube/config.json ...
I0812 13:37:43.327962   24335 lock.go:35] WriteFile acquiring /Users/banan/.minikube/profiles/minikube/config.json: {Name:mk843deb6c80d00dc0b2c002de3742d04c09d94a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:37:43.411597   24335 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0812 13:37:43.411612   24335 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I0812 13:37:43.411632   24335 cache.go:195] Successfully downloaded all kic artifacts
I0812 13:37:43.411688   24335 start.go:365] acquiring machines lock for minikube: {Name:mkde87ddaa6516452cdd107b7fcf265c2295f0e7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0812 13:37:43.411832   24335 start.go:369] acquired machines lock for "minikube" in 126.535µs
I0812 13:37:43.411862   24335 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0} &{Name: IP: Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0812 13:37:43.411968   24335 start.go:125] createHost starting for "" (driver="docker")
I0812 13:37:43.415612   24335 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=4000MB) ...
I0812 13:37:43.415845   24335 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0812 13:37:43.415878   24335 client.go:168] LocalClient.Create starting
I0812 13:37:43.416120   24335 main.go:141] libmachine: Reading certificate data from /Users/banan/.minikube/certs/ca.pem
I0812 13:37:43.416497   24335 main.go:141] libmachine: Decoding PEM data...
I0812 13:37:43.416525   24335 main.go:141] libmachine: Parsing certificate...
I0812 13:37:43.416631   24335 main.go:141] libmachine: Reading certificate data from /Users/banan/.minikube/certs/cert.pem
I0812 13:37:43.416926   24335 main.go:141] libmachine: Decoding PEM data...
I0812 13:37:43.416955   24335 main.go:141] libmachine: Parsing certificate...
I0812 13:37:43.420905   24335 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0812 13:37:43.494950   24335 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0812 13:37:43.495134   24335 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0812 13:37:43.495159   24335 cli_runner.go:164] Run: docker network inspect minikube
W0812 13:37:43.567574   24335 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0812 13:37:43.567611   24335 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0812 13:37:43.567626   24335 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0812 13:37:43.567754   24335 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0812 13:37:43.639618   24335 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc000cb4ff0}
I0812 13:37:43.639656   24335 network_create.go:123] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0812 13:37:43.639759   24335 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0812 13:37:43.766833   24335 network_create.go:107] docker network minikube 192.168.49.0/24 created
I0812 13:37:43.766880   24335 kic.go:117] calculated static IP "192.168.49.2" for the "minikube" container
I0812 13:37:43.767061   24335 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0812 13:37:43.855138   24335 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0812 13:37:43.929998   24335 oci.go:103] Successfully created a docker volume minikube
I0812 13:37:43.930197   24335 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib
I0812 13:37:44.934865   24335 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib: (1.004566298s)
I0812 13:37:44.934907   24335 oci.go:107] Successfully prepared a docker volume minikube
I0812 13:37:44.934957   24335 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0812 13:37:44.934987   24335 kic.go:190] Starting extracting preloaded images to volume ...
I0812 13:37:44.935148   24335 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/banan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir
I0812 13:37:56.613792   24335 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/banan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir: (11.678743751s)
I0812 13:37:56.613821   24335 kic.go:199] duration metric: took 11.679024 seconds to extract preloaded images to volume
I0812 13:37:56.614018   24335 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0812 13:37:56.765750   24335 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631
I0812 13:37:57.132849   24335 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0812 13:37:57.208938   24335 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0812 13:37:57.288332   24335 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0812 13:37:57.432597   24335 oci.go:144] the created container "minikube" has a running status.
I0812 13:37:57.432650   24335 kic.go:221] Creating ssh key for kic: /Users/banan/.minikube/machines/minikube/id_rsa...
I0812 13:37:57.501794   24335 kic_runner.go:191] docker (temp): /Users/banan/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0812 13:37:57.591431   24335 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0812 13:37:57.672294   24335 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0812 13:37:57.672314   24335 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0812 13:37:57.841766   24335 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0812 13:37:57.924232   24335 machine.go:88] provisioning docker machine ...
I0812 13:37:57.924330   24335 ubuntu.go:169] provisioning hostname "minikube"
I0812 13:37:57.924541   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:37:58.004006   24335 main.go:141] libmachine: Using SSH client type: native
I0812 13:37:58.004669   24335 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100411a40] 0x100414ae0 <nil>  [] 0s} 127.0.0.1 55875 <nil> <nil>}
I0812 13:37:58.004685   24335 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0812 13:37:58.316405   24335 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0812 13:37:58.316561   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:37:58.400837   24335 main.go:141] libmachine: Using SSH client type: native
I0812 13:37:58.401423   24335 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100411a40] 0x100414ae0 <nil>  [] 0s} 127.0.0.1 55875 <nil> <nil>}
I0812 13:37:58.401442   24335 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0812 13:37:58.635369   24335 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0812 13:37:58.635398   24335 ubuntu.go:175] set auth options {CertDir:/Users/banan/.minikube CaCertPath:/Users/banan/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/banan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/banan/.minikube/machines/server.pem ServerKeyPath:/Users/banan/.minikube/machines/server-key.pem ClientKeyPath:/Users/banan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/banan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/banan/.minikube}
I0812 13:37:58.635434   24335 ubuntu.go:177] setting up certificates
I0812 13:37:58.635442   24335 provision.go:83] configureAuth start
I0812 13:37:58.635560   24335 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0812 13:37:58.712035   24335 provision.go:138] copyHostCerts
I0812 13:37:58.712197   24335 exec_runner.go:144] found /Users/banan/.minikube/ca.pem, removing ...
I0812 13:37:58.712204   24335 exec_runner.go:203] rm: /Users/banan/.minikube/ca.pem
I0812 13:37:58.712388   24335 exec_runner.go:151] cp: /Users/banan/.minikube/certs/ca.pem --> /Users/banan/.minikube/ca.pem (1074 bytes)
I0812 13:37:58.712771   24335 exec_runner.go:144] found /Users/banan/.minikube/cert.pem, removing ...
I0812 13:37:58.712775   24335 exec_runner.go:203] rm: /Users/banan/.minikube/cert.pem
I0812 13:37:58.712873   24335 exec_runner.go:151] cp: /Users/banan/.minikube/certs/cert.pem --> /Users/banan/.minikube/cert.pem (1119 bytes)
I0812 13:37:58.713090   24335 exec_runner.go:144] found /Users/banan/.minikube/key.pem, removing ...
I0812 13:37:58.713094   24335 exec_runner.go:203] rm: /Users/banan/.minikube/key.pem
I0812 13:37:58.713180   24335 exec_runner.go:151] cp: /Users/banan/.minikube/certs/key.pem --> /Users/banan/.minikube/key.pem (1679 bytes)
I0812 13:37:58.713619   24335 provision.go:112] generating server cert: /Users/banan/.minikube/machines/server.pem ca-key=/Users/banan/.minikube/certs/ca.pem private-key=/Users/banan/.minikube/certs/ca-key.pem org=banan.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0812 13:37:58.799856   24335 provision.go:172] copyRemoteCerts
I0812 13:37:58.800474   24335 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0812 13:37:58.800613   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:37:58.877139   24335 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55875 SSHKeyPath:/Users/banan/.minikube/machines/minikube/id_rsa Username:docker}
I0812 13:37:58.986095   24335 ssh_runner.go:362] scp /Users/banan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0812 13:37:59.020885   24335 ssh_runner.go:362] scp /Users/banan/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I0812 13:37:59.054364   24335 ssh_runner.go:362] scp /Users/banan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0812 13:37:59.084221   24335 provision.go:86] duration metric: configureAuth took 448.761669ms
I0812 13:37:59.084238   24335 ubuntu.go:193] setting minikube options for container-runtime
I0812 13:37:59.084509   24335 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0812 13:37:59.084612   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:37:59.164837   24335 main.go:141] libmachine: Using SSH client type: native
I0812 13:37:59.165374   24335 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100411a40] 0x100414ae0 <nil>  [] 0s} 127.0.0.1 55875 <nil> <nil>}
I0812 13:37:59.165387   24335 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0812 13:37:59.309837   24335 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0812 13:37:59.309854   24335 ubuntu.go:71] root file system type: overlay
I0812 13:37:59.309996   24335 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0812 13:37:59.310108   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:37:59.386376   24335 main.go:141] libmachine: Using SSH client type: native
I0812 13:37:59.386921   24335 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100411a40] 0x100414ae0 <nil>  [] 0s} 127.0.0.1 55875 <nil> <nil>}
I0812 13:37:59.386995   24335 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0812 13:37:59.554755   24335 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0812 13:37:59.554925   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:37:59.634551   24335 main.go:141] libmachine: Using SSH client type: native
I0812 13:37:59.635171   24335 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100411a40] 0x100414ae0 <nil>  [] 0s} 127.0.0.1 55875 <nil> <nil>}
I0812 13:37:59.635192   24335 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0812 13:38:00.548635   24335 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-07-07 14:50:55.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-08-12 20:37:59.553046904 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0812 13:38:00.548664   24335 machine.go:91] provisioned docker machine in 2.624443419s
I0812 13:38:00.548675   24335 client.go:171] LocalClient.Create took 17.13307012s
I0812 13:38:00.548724   24335 start.go:167] duration metric: libmachine.API.Create for "minikube" took 17.133152942s
I0812 13:38:00.548737   24335 start.go:300] post-start starting for "minikube" (driver="docker")
I0812 13:38:00.548760   24335 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0812 13:38:00.548959   24335 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0812 13:38:00.549050   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:38:00.631288   24335 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55875 SSHKeyPath:/Users/banan/.minikube/machines/minikube/id_rsa Username:docker}
I0812 13:38:00.738823   24335 ssh_runner.go:195] Run: cat /etc/os-release
I0812 13:38:00.744983   24335 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0812 13:38:00.745014   24335 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0812 13:38:00.745022   24335 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0812 13:38:00.745030   24335 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0812 13:38:00.745041   24335 filesync.go:126] Scanning /Users/banan/.minikube/addons for local assets ...
I0812 13:38:00.745280   24335 filesync.go:126] Scanning /Users/banan/.minikube/files for local assets ...
I0812 13:38:00.745364   24335 start.go:303] post-start completed in 196.624064ms
I0812 13:38:00.746024   24335 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0812 13:38:00.820557   24335 profile.go:148] Saving config to /Users/banan/.minikube/profiles/minikube/config.json ...
I0812 13:38:00.821213   24335 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0812 13:38:00.821315   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:38:00.895443   24335 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55875 SSHKeyPath:/Users/banan/.minikube/machines/minikube/id_rsa Username:docker}
I0812 13:38:01.006156   24335 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0812 13:38:01.014605   24335 start.go:128] duration metric: createHost completed in 17.602904368s
I0812 13:38:01.014630   24335 start.go:83] releasing machines lock for "minikube", held for 17.60306794s
I0812 13:38:01.014781   24335 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0812 13:38:01.091543   24335 ssh_runner.go:195] Run: cat /version.json
I0812 13:38:01.091630   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:38:01.092385   24335 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0812 13:38:01.092639   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:38:01.174992   24335 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55875 SSHKeyPath:/Users/banan/.minikube/machines/minikube/id_rsa Username:docker}
I0812 13:38:01.175698   24335 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55875 SSHKeyPath:/Users/banan/.minikube/machines/minikube/id_rsa Username:docker}
I0812 13:38:01.681573   24335 ssh_runner.go:195] Run: systemctl --version
I0812 13:38:01.690167   24335 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0812 13:38:01.700263   24335 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0812 13:38:01.744678   24335 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0812 13:38:01.744811   24335 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0812 13:38:01.780858   24335 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0812 13:38:01.780874   24335 start.go:466] detecting cgroup driver to use...
I0812 13:38:01.780889   24335 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0812 13:38:01.781015   24335 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0812 13:38:01.804125   24335 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0812 13:38:01.818271   24335 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0812 13:38:01.832005   24335 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0812 13:38:01.832129   24335 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0812 13:38:01.846420   24335 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0812 13:38:01.861514   24335 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0812 13:38:01.876242   24335 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0812 13:38:01.890871   24335 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0812 13:38:01.905132   24335 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0812 13:38:01.919318   24335 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0812 13:38:01.931909   24335 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0812 13:38:01.945245   24335 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0812 13:38:02.044250   24335 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0812 13:38:02.141186   24335 start.go:466] detecting cgroup driver to use...
I0812 13:38:02.141204   24335 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0812 13:38:02.141353   24335 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0812 13:38:02.160367   24335 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0812 13:38:02.160486   24335 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0812 13:38:02.183094   24335 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0812 13:38:02.212460   24335 ssh_runner.go:195] Run: which cri-dockerd
I0812 13:38:02.220603   24335 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0812 13:38:02.236388   24335 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0812 13:38:02.261039   24335 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0812 13:38:02.352520   24335 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0812 13:38:02.457900   24335 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0812 13:38:02.457934   24335 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0812 13:38:02.496767   24335 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0812 13:38:02.598344   24335 ssh_runner.go:195] Run: sudo systemctl restart docker
I0812 13:38:02.948546   24335 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0812 13:38:03.051255   24335 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0812 13:38:03.152969   24335 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0812 13:38:03.254954   24335 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0812 13:38:03.350887   24335 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0812 13:38:03.379834   24335 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0812 13:38:03.477928   24335 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0812 13:38:03.697715   24335 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0812 13:38:03.698185   24335 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0812 13:38:03.705416   24335 start.go:534] Will wait 60s for crictl version
I0812 13:38:03.705516   24335 ssh_runner.go:195] Run: which crictl
I0812 13:38:03.712131   24335 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0812 13:38:03.779305   24335 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0812 13:38:03.779428   24335 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0812 13:38:03.815190   24335 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0812 13:38:03.860581   24335 out.go:204] 🐳  Подготавливается Kubernetes v1.27.3 на Docker 24.0.4 ...
I0812 13:38:03.860745   24335 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0812 13:38:04.099337   24335 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0812 13:38:04.100261   24335 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0812 13:38:04.107171   24335 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0812 13:38:04.123726   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0812 13:38:04.198986   24335 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0812 13:38:04.199112   24335 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0812 13:38:04.225734   24335 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0812 13:38:04.225835   24335 docker.go:566] Images already preloaded, skipping extraction
I0812 13:38:04.226011   24335 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0812 13:38:04.254292   24335 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0812 13:38:04.254318   24335 cache_images.go:84] Images are preloaded, skipping loading
I0812 13:38:04.254437   24335 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0812 13:38:04.446365   24335 cni.go:84] Creating CNI manager for ""
I0812 13:38:04.446381   24335 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0812 13:38:04.446400   24335 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0812 13:38:04.446425   24335 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0812 13:38:04.446604   24335 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0812 13:38:04.446707   24335 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0812 13:38:04.446830   24335 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.3
I0812 13:38:04.458988   24335 binaries.go:44] Found k8s binaries, skipping transfer
I0812 13:38:04.459109   24335 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0812 13:38:04.472201   24335 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0812 13:38:04.496607   24335 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0812 13:38:04.521984   24335 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0812 13:38:04.551510   24335 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0812 13:38:04.558502   24335 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0812 13:38:04.575913   24335 certs.go:56] Setting up /Users/banan/.minikube/profiles/minikube for IP: 192.168.49.2
I0812 13:38:04.575942   24335 certs.go:190] acquiring lock for shared ca certs: {Name:mkacf44a8cd14a10ec4c6f6dfa4faf619767d83d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:04.576572   24335 certs.go:199] skipping minikubeCA CA generation: /Users/banan/.minikube/ca.key
I0812 13:38:04.576966   24335 certs.go:199] skipping proxyClientCA CA generation: /Users/banan/.minikube/proxy-client-ca.key
I0812 13:38:04.577065   24335 certs.go:319] generating minikube-user signed cert: /Users/banan/.minikube/profiles/minikube/client.key
I0812 13:38:04.577645   24335 crypto.go:68] Generating cert /Users/banan/.minikube/profiles/minikube/client.crt with IP's: []
I0812 13:38:04.708690   24335 crypto.go:156] Writing cert to /Users/banan/.minikube/profiles/minikube/client.crt ...
I0812 13:38:04.708706   24335 lock.go:35] WriteFile acquiring /Users/banan/.minikube/profiles/minikube/client.crt: {Name:mk35e70d39b1dd6ffc426d96fc0d39bcdce31e81 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:04.709755   24335 crypto.go:164] Writing key to /Users/banan/.minikube/profiles/minikube/client.key ...
I0812 13:38:04.709766   24335 lock.go:35] WriteFile acquiring /Users/banan/.minikube/profiles/minikube/client.key: {Name:mk25e768d0e0262d4b0f944c6b58b96ed775c646 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:04.710076   24335 certs.go:319] generating minikube signed cert: /Users/banan/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0812 13:38:04.710110   24335 crypto.go:68] Generating cert /Users/banan/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0812 13:38:04.882099   24335 crypto.go:156] Writing cert to /Users/banan/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I0812 13:38:04.882111   24335 lock.go:35] WriteFile acquiring /Users/banan/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mkd55a27402288a68515281fca8d8021006664d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:04.883367   24335 crypto.go:164] Writing key to /Users/banan/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I0812 13:38:04.883391   24335 lock.go:35] WriteFile acquiring /Users/banan/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mkfa5220821d443e9694c1761e24c13c7e103ffa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:04.883682   24335 certs.go:337] copying /Users/banan/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /Users/banan/.minikube/profiles/minikube/apiserver.crt
I0812 13:38:04.884332   24335 certs.go:341] copying /Users/banan/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /Users/banan/.minikube/profiles/minikube/apiserver.key
I0812 13:38:04.884583   24335 certs.go:319] generating aggregator signed cert: /Users/banan/.minikube/profiles/minikube/proxy-client.key
I0812 13:38:04.884614   24335 crypto.go:68] Generating cert /Users/banan/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0812 13:38:04.979636   24335 crypto.go:156] Writing cert to /Users/banan/.minikube/profiles/minikube/proxy-client.crt ...
I0812 13:38:04.979649   24335 lock.go:35] WriteFile acquiring /Users/banan/.minikube/profiles/minikube/proxy-client.crt: {Name:mk564159c023cbdc9e5917662ec26dedfc9b4ab3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:04.980044   24335 crypto.go:164] Writing key to /Users/banan/.minikube/profiles/minikube/proxy-client.key ...
I0812 13:38:04.980052   24335 lock.go:35] WriteFile acquiring /Users/banan/.minikube/profiles/minikube/proxy-client.key: {Name:mk72c691af6cba994cef47fab58561f80f9fb63d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:04.980672   24335 certs.go:437] found cert: /Users/banan/.minikube/certs/Users/banan/.minikube/certs/ca-key.pem (1679 bytes)
I0812 13:38:04.980728   24335 certs.go:437] found cert: /Users/banan/.minikube/certs/Users/banan/.minikube/certs/ca.pem (1074 bytes)
I0812 13:38:04.980777   24335 certs.go:437] found cert: /Users/banan/.minikube/certs/Users/banan/.minikube/certs/cert.pem (1119 bytes)
I0812 13:38:04.980820   24335 certs.go:437] found cert: /Users/banan/.minikube/certs/Users/banan/.minikube/certs/key.pem (1679 bytes)
I0812 13:38:04.983131   24335 ssh_runner.go:362] scp /Users/banan/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0812 13:38:05.015169   24335 ssh_runner.go:362] scp /Users/banan/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0812 13:38:05.047574   24335 ssh_runner.go:362] scp /Users/banan/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0812 13:38:05.078114   24335 ssh_runner.go:362] scp /Users/banan/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0812 13:38:05.108375   24335 ssh_runner.go:362] scp /Users/banan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0812 13:38:05.140778   24335 ssh_runner.go:362] scp /Users/banan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0812 13:38:05.171069   24335 ssh_runner.go:362] scp /Users/banan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0812 13:38:05.202300   24335 ssh_runner.go:362] scp /Users/banan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0812 13:38:05.235524   24335 ssh_runner.go:362] scp /Users/banan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0812 13:38:05.266306   24335 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0812 13:38:05.290594   24335 ssh_runner.go:195] Run: openssl version
I0812 13:38:05.301244   24335 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0812 13:38:05.318474   24335 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0812 13:38:05.326233   24335 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Aug 10 23:43 /usr/share/ca-certificates/minikubeCA.pem
I0812 13:38:05.326373   24335 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0812 13:38:05.336899   24335 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0812 13:38:05.350410   24335 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0812 13:38:05.355772   24335 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0812 13:38:05.355846   24335 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0812 13:38:05.355987   24335 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0812 13:38:05.379898   24335 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0812 13:38:05.392408   24335 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0812 13:38:05.405307   24335 kubeadm.go:226] ignoring SystemVerification for kubeadm because of docker driver
I0812 13:38:05.405433   24335 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0812 13:38:05.418212   24335 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0812 13:38:05.418250   24335 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0812 13:38:05.482795   24335 kubeadm.go:322] [init] Using Kubernetes version: v1.27.3
I0812 13:38:05.482865   24335 kubeadm.go:322] [preflight] Running pre-flight checks
I0812 13:38:05.657371   24335 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0812 13:38:05.657563   24335 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0812 13:38:05.657706   24335 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0812 13:38:06.096678   24335 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0812 13:38:06.106019   24335 out.go:204]     ▪ Generating certificates and keys ...
I0812 13:38:06.107356   24335 kubeadm.go:322] [certs] Using existing ca certificate authority
I0812 13:38:06.107467   24335 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0812 13:38:06.242254   24335 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I0812 13:38:06.640726   24335 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I0812 13:38:06.780297   24335 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I0812 13:38:07.007706   24335 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I0812 13:38:07.307077   24335 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I0812 13:38:07.307309   24335 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0812 13:38:07.539206   24335 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I0812 13:38:07.539400   24335 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0812 13:38:07.872015   24335 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I0812 13:38:08.239750   24335 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I0812 13:38:08.411642   24335 kubeadm.go:322] [certs] Generating "sa" key and public key
I0812 13:38:08.412245   24335 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0812 13:38:09.280677   24335 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I0812 13:38:09.645606   24335 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0812 13:38:09.858773   24335 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0812 13:38:10.180785   24335 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0812 13:38:10.200829   24335 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0812 13:38:10.202218   24335 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0812 13:38:10.202286   24335 kubeadm.go:322] [kubelet-start] Starting the kubelet
I0812 13:38:10.305895   24335 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0812 13:38:10.319353   24335 out.go:204]     ▪ Booting up control plane ...
I0812 13:38:10.319607   24335 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0812 13:38:10.319729   24335 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0812 13:38:10.319847   24335 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0812 13:38:10.320025   24335 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0812 13:38:10.320218   24335 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0812 13:38:17.315362   24335 kubeadm.go:322] [apiclient] All control plane components are healthy after 7.002486 seconds
I0812 13:38:17.315614   24335 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0812 13:38:17.330437   24335 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0812 13:38:17.864724   24335 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I0812 13:38:17.864984   24335 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0812 13:38:18.379355   24335 kubeadm.go:322] [bootstrap-token] Using token: 0ofjyy.rfrmg8o02q2tpay9
I0812 13:38:18.382802   24335 out.go:204]     ▪ Configuring RBAC rules ...
I0812 13:38:18.383004   24335 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0812 13:38:18.391477   24335 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0812 13:38:18.405221   24335 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0812 13:38:18.412371   24335 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0812 13:38:18.421298   24335 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0812 13:38:18.424614   24335 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0812 13:38:18.441000   24335 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0812 13:38:18.708431   24335 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I0812 13:38:18.800782   24335 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I0812 13:38:18.802840   24335 kubeadm.go:322] 
I0812 13:38:18.803061   24335 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I0812 13:38:18.803071   24335 kubeadm.go:322] 
I0812 13:38:18.803185   24335 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I0812 13:38:18.803203   24335 kubeadm.go:322] 
I0812 13:38:18.803291   24335 kubeadm.go:322]   mkdir -p $HOME/.kube
I0812 13:38:18.803368   24335 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0812 13:38:18.803500   24335 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0812 13:38:18.803508   24335 kubeadm.go:322] 
I0812 13:38:18.803563   24335 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I0812 13:38:18.803566   24335 kubeadm.go:322] 
I0812 13:38:18.803611   24335 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0812 13:38:18.803627   24335 kubeadm.go:322] 
I0812 13:38:18.803731   24335 kubeadm.go:322] You should now deploy a pod network to the cluster.
I0812 13:38:18.803843   24335 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0812 13:38:18.803928   24335 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0812 13:38:18.803935   24335 kubeadm.go:322] 
I0812 13:38:18.804027   24335 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I0812 13:38:18.804130   24335 kubeadm.go:322] and service account keys on each node and then running the following as root:
I0812 13:38:18.804133   24335 kubeadm.go:322] 
I0812 13:38:18.804216   24335 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token 0ofjyy.rfrmg8o02q2tpay9 \
I0812 13:38:18.804357   24335 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:01f28651fbbeed1b4bf9c1f531de2544d811f0d165d6fd0574afba3c69b3f9d4 \
I0812 13:38:18.804384   24335 kubeadm.go:322] 	--control-plane 
I0812 13:38:18.804387   24335 kubeadm.go:322] 
I0812 13:38:18.804478   24335 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I0812 13:38:18.804486   24335 kubeadm.go:322] 
I0812 13:38:18.804592   24335 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token 0ofjyy.rfrmg8o02q2tpay9 \
I0812 13:38:18.804721   24335 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:01f28651fbbeed1b4bf9c1f531de2544d811f0d165d6fd0574afba3c69b3f9d4 
I0812 13:38:18.808331   24335 kubeadm.go:322] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0812 13:38:18.808505   24335 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0812 13:38:18.808525   24335 cni.go:84] Creating CNI manager for ""
I0812 13:38:18.808537   24335 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0812 13:38:18.813589   24335 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0812 13:38:18.826273   24335 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0812 13:38:18.844157   24335 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0812 13:38:18.909639   24335 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0812 13:38:18.909857   24335 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.3/kubectl label nodes minikube.k8s.io/version=v1.31.1 minikube.k8s.io/commit=fd3f3801765d093a485d255043149f92ec0a695f minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_08_12T13_38_18_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0812 13:38:18.909953   24335 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0812 13:38:19.308045   24335 ops.go:34] apiserver oom_adj: -16
I0812 13:38:19.308072   24335 kubeadm.go:1081] duration metric: took 398.222703ms to wait for elevateKubeSystemPrivileges.
I0812 13:38:19.308116   24335 kubeadm.go:406] StartCluster complete in 13.952463664s
I0812 13:38:19.308145   24335 settings.go:142] acquiring lock: {Name:mk5bb4b73ea43310bbdd174cbc4c2f1d7d23dbfb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:19.308285   24335 settings.go:150] Updating kubeconfig:  /Users/banan/.kube/config
I0812 13:38:19.309239   24335 lock.go:35] WriteFile acquiring /Users/banan/.kube/config: {Name:mk6ffe81d6cee76311ce9a96a6ac3694b8a37979 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0812 13:38:19.310104   24335 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0812 13:38:19.310163   24335 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0812 13:38:19.310281   24335 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0812 13:38:19.310307   24335 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0812 13:38:19.310304   24335 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0812 13:38:19.310339   24335 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0812 13:38:19.310360   24335 host.go:66] Checking if "minikube" exists ...
I0812 13:38:19.310354   24335 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0812 13:38:19.310884   24335 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0812 13:38:19.310906   24335 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0812 13:38:19.403161   24335 out.go:177]     ▪ Используется образ gcr.io/k8s-minikube/storage-provisioner:v5
I0812 13:38:19.408986   24335 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0812 13:38:19.410704   24335 host.go:66] Checking if "minikube" exists ...
I0812 13:38:19.410751   24335 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0812 13:38:19.410758   24335 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0812 13:38:19.410853   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:38:19.412030   24335 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0812 13:38:19.493874   24335 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0812 13:38:19.496652   24335 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0812 13:38:19.496663   24335 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0812 13:38:19.496847   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0812 13:38:19.511303   24335 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55875 SSHKeyPath:/Users/banan/.minikube/machines/minikube/id_rsa Username:docker}
I0812 13:38:19.593872   24335 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55875 SSHKeyPath:/Users/banan/.minikube/machines/minikube/id_rsa Username:docker}
I0812 13:38:19.689403   24335 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0812 13:38:19.816338   24335 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0812 13:38:19.877663   24335 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0812 13:38:19.877696   24335 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0812 13:38:19.891722   24335 out.go:177] 🔎  Компоненты Kubernetes проверяются ...
I0812 13:38:19.914698   24335 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0812 13:38:21.095398   24335 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.601494144s)
I0812 13:38:21.095430   24335 start.go:901] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0812 13:38:21.279431   24335 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.590008745s)
I0812 13:38:21.279544   24335 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.463199111s)
I0812 13:38:21.279584   24335 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.36488554s)
I0812 13:38:21.279737   24335 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0812 13:38:21.285967   24335 out.go:177] 🌟  Включенные дополнения: storage-provisioner, default-storageclass
I0812 13:38:21.304248   24335 addons.go:502] enable addons completed in 1.99409573s: enabled=[storage-provisioner default-storageclass]
I0812 13:38:21.364732   24335 api_server.go:52] waiting for apiserver process to appear ...
I0812 13:38:21.364847   24335 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0812 13:38:21.382120   24335 api_server.go:72] duration metric: took 1.504413245s to wait for apiserver process to appear ...
I0812 13:38:21.382142   24335 api_server.go:88] waiting for apiserver healthz status ...
I0812 13:38:21.382161   24335 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55874/healthz ...
I0812 13:38:21.390403   24335 api_server.go:279] https://127.0.0.1:55874/healthz returned 200:
ok
I0812 13:38:21.392494   24335 api_server.go:141] control plane version: v1.27.3
I0812 13:38:21.392505   24335 api_server.go:131] duration metric: took 10.357528ms to wait for apiserver health ...
I0812 13:38:21.392518   24335 system_pods.go:43] waiting for kube-system pods to appear ...
I0812 13:38:21.405675   24335 system_pods.go:59] 5 kube-system pods found
I0812 13:38:21.405691   24335 system_pods.go:61] "etcd-minikube" [44215ac2-39ff-4078-9164-9ec38043c6fe] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0812 13:38:21.405699   24335 system_pods.go:61] "kube-apiserver-minikube" [d89576df-4e7b-484a-a242-3051fa29b4a2] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0812 13:38:21.405708   24335 system_pods.go:61] "kube-controller-manager-minikube" [b5d74537-6f26-4866-a308-41a4c4387e65] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0812 13:38:21.405714   24335 system_pods.go:61] "kube-scheduler-minikube" [8e9c9e06-152b-4a2a-b2d1-8651850ae79f] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0812 13:38:21.405719   24335 system_pods.go:61] "storage-provisioner" [6e96bd61-c362-4709-b1e1-9ac79e398bae] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..)
I0812 13:38:21.405724   24335 system_pods.go:74] duration metric: took 13.202065ms to wait for pod list to return data ...
I0812 13:38:21.405731   24335 kubeadm.go:581] duration metric: took 1.528029409s to wait for : map[apiserver:true system_pods:true] ...
I0812 13:38:21.405741   24335 node_conditions.go:102] verifying NodePressure condition ...
I0812 13:38:21.411381   24335 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0812 13:38:21.411400   24335 node_conditions.go:123] node cpu capacity is 4
I0812 13:38:21.411409   24335 node_conditions.go:105] duration metric: took 5.665146ms to run NodePressure ...
I0812 13:38:21.411419   24335 start.go:228] waiting for startup goroutines ...
I0812 13:38:21.411424   24335 start.go:233] waiting for cluster config update ...
I0812 13:38:21.411436   24335 start.go:242] writing updated cluster config ...
I0812 13:38:21.412515   24335 ssh_runner.go:195] Run: rm -f paused
I0812 13:38:21.900988   24335 start.go:596] kubectl: 1.27.2, cluster: 1.27.3 (minor skew: 0)
I0812 13:38:21.908312   24335 out.go:177] 🏄  Готово! kubectl настроен для использования кластера "minikube" и "default" пространства имён по умолчанию

* 
* ==> Docker <==
* Aug 12 20:38:02 minikube dockerd[1027]: time="2023-08-12T20:38:02.911383384Z" level=info msg="Docker daemon" commit=4ffc614 graphdriver=overlay2 version=24.0.4
Aug 12 20:38:02 minikube dockerd[1027]: time="2023-08-12T20:38:02.911465765Z" level=info msg="Daemon has completed initialization"
Aug 12 20:38:02 minikube dockerd[1027]: time="2023-08-12T20:38:02.950877757Z" level=info msg="API listen on [::]:2376"
Aug 12 20:38:02 minikube dockerd[1027]: time="2023-08-12T20:38:02.950928592Z" level=info msg="API listen on /var/run/docker.sock"
Aug 12 20:38:02 minikube systemd[1]: Started Docker Application Container Engine.
Aug 12 20:38:03 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Start docker client with request timeout 0s"
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Hairpin mode is set to hairpin-veth"
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Loaded network plugin cni"
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Docker cri networking managed by network plugin cni"
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Docker Info: &{ID:1b32cc99-a59a-4f3b-b724-30b6a893be39 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:false NGoroutines:35 SystemTime:2023-08-12T20:38:03.687498687Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:2 NEventsListener:0 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Ubuntu 22.04.2 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc00027e3f0 NCPU:4 MemTotal:8241242112 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Setting cgroupDriver cgroupfs"
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Aug 12 20:38:03 minikube cri-dockerd[1246]: time="2023-08-12T20:38:03Z" level=info msg="Start cri-dockerd grpc backend"
Aug 12 20:38:03 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 12 20:38:12 minikube cri-dockerd[1246]: time="2023-08-12T20:38:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/83055203456c60af15ab91231f51c6911a396adabd5297d3115caec5e93abd40/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 12 20:38:12 minikube cri-dockerd[1246]: time="2023-08-12T20:38:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ca3211651bfa5f62ebaa6cef844390edf16361b5a0b1021d3596d909d9eac6b7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 12 20:38:12 minikube cri-dockerd[1246]: time="2023-08-12T20:38:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1d3e8a73d6f1e5423c46af550592c3e9034ac16ccb62096ce19e1862109e357e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 12 20:38:12 minikube cri-dockerd[1246]: time="2023-08-12T20:38:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68ae9e435cb696e94a779ae46c5c9bcfb05e80f5d89ccf9fd20e651f3631ea75/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 12 20:38:31 minikube cri-dockerd[1246]: time="2023-08-12T20:38:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b98517a7304ef7dc89aa9499247b6db2e6a833c20c0b804b6455f0ba89ff5016/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 12 20:38:32 minikube cri-dockerd[1246]: time="2023-08-12T20:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cb3a401369abff479a5d8c90da859bbe4b35f6a448d2875e0d86e17e59c6af9c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 12 20:38:32 minikube cri-dockerd[1246]: time="2023-08-12T20:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2e6b82ae59f1b08e201d89cd0c887c6201a8e26352cf4a7c5a97de090437d5ac/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 12 20:38:39 minikube cri-dockerd[1246]: time="2023-08-12T20:38:39Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 12 20:39:01 minikube dockerd[1027]: time="2023-08-12T20:39:01.981376352Z" level=info msg="ignoring event" container=622196d76c398fc4b1d30d2b02b4e6509526d698495b150bfd144b65f11dc019 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 12 20:43:02 minikube cri-dockerd[1246]: time="2023-08-12T20:43:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc44b471b37730f134ea33adfcaf6298d4ad19c18ff2b2e773a44445acaa3941/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 12 20:43:03 minikube cri-dockerd[1246]: time="2023-08-12T20:43:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c27ea6de8d3bee002bdd9077ce0b442ea0ea17d0e6741d3875b68d24a1dc3a39/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 12 20:43:03 minikube cri-dockerd[1246]: time="2023-08-12T20:43:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9f154da29fea256d64c6026b485efa831e820a3d95bb3f90f0b3ef3a41794c26/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 12 20:43:14 minikube cri-dockerd[1246]: time="2023-08-12T20:43:14Z" level=info msg="Pulling image docker.io/antonbanan/todo-front:latest: 751194035c36: Downloading [================================>                  ]  30.83MB/47.76MB"
Aug 12 20:43:24 minikube cri-dockerd[1246]: time="2023-08-12T20:43:24Z" level=info msg="Pulling image docker.io/antonbanan/todo-front:latest: 751194035c36: Extracting [================================================>  ]  46.69MB/47.76MB"
Aug 12 20:43:34 minikube cri-dockerd[1246]: time="2023-08-12T20:43:34Z" level=info msg="Pulling image docker.io/antonbanan/todo-front:latest: abf1bb787866: Downloading [====================================>              ]    235MB/324.2MB"
Aug 12 20:43:44 minikube cri-dockerd[1246]: time="2023-08-12T20:43:44Z" level=info msg="Pulling image docker.io/antonbanan/todo-front:latest: abf1bb787866: Downloading [=======================================>           ]  254.4MB/324.2MB"
Aug 12 20:43:45 minikube dockerd[1027]: time="2023-08-12T20:43:45.451425386Z" level=info msg="ignoring event" container=cca015af045f1960e57483a9821357f3c943d8f0b4251194441ddcac6393185f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 12 20:43:54 minikube cri-dockerd[1246]: time="2023-08-12T20:43:54Z" level=info msg="Pulling image docker.io/antonbanan/todo-front:latest: abf1bb787866: Downloading [==========================================>        ]  278.1MB/324.2MB"
Aug 12 20:44:04 minikube cri-dockerd[1246]: time="2023-08-12T20:44:04Z" level=info msg="Pulling image docker.io/antonbanan/todo-front:latest: abf1bb787866: Extracting [==========>                                        ]  65.18MB/324.2MB"
Aug 12 20:44:14 minikube cri-dockerd[1246]: time="2023-08-12T20:44:14Z" level=info msg="Pulling image docker.io/antonbanan/todo-front:latest: abf1bb787866: Extracting [====================>                              ]  134.8MB/324.2MB"
Aug 12 20:44:23 minikube cri-dockerd[1246]: time="2023-08-12T20:44:23Z" level=info msg="Stop pulling image docker.io/antonbanan/todo-front:latest: Status: Downloaded newer image for antonbanan/todo-front:latest"
Aug 12 20:44:32 minikube cri-dockerd[1246]: time="2023-08-12T20:44:32Z" level=info msg="Stop pulling image docker.io/antonbanan/todo-app:latest: Status: Downloaded newer image for antonbanan/todo-app:latest"
Aug 12 20:44:43 minikube cri-dockerd[1246]: time="2023-08-12T20:44:43Z" level=info msg="Pulling image mongo:latest: 865b2fb03178: Downloading [==============>                                    ]  57.06MB/200.8MB"
Aug 12 20:44:53 minikube cri-dockerd[1246]: time="2023-08-12T20:44:53Z" level=info msg="Pulling image mongo:latest: 865b2fb03178: Downloading [=======================================>           ]  159.8MB/200.8MB"
Aug 12 20:45:03 minikube cri-dockerd[1246]: time="2023-08-12T20:45:03Z" level=info msg="Pulling image mongo:latest: 865b2fb03178: Extracting [==============================>                    ]    122MB/200.8MB"
Aug 12 20:45:07 minikube cri-dockerd[1246]: time="2023-08-12T20:45:07Z" level=info msg="Stop pulling image mongo:latest: Status: Downloaded newer image for mongo:latest"
Aug 12 20:51:10 minikube dockerd[1027]: time="2023-08-12T20:51:10.895971040Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f
Aug 12 20:51:10 minikube dockerd[1027]: time="2023-08-12T20:51:10.981381463Z" level=info msg="ignoring event" container=94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 12 20:51:11 minikube dockerd[1027]: time="2023-08-12T20:51:11.146779729Z" level=info msg="ignoring event" container=cc44b471b37730f134ea33adfcaf6298d4ad19c18ff2b2e773a44445acaa3941 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 12 20:51:23 minikube cri-dockerd[1246]: time="2023-08-12T20:51:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f6b77a96f997754146235e7ff38e8cdb4f17ab9a953a805514c3a80475c2a112/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 12 20:51:24 minikube cri-dockerd[1246]: time="2023-08-12T20:51:24Z" level=info msg="Stop pulling image docker.io/antonbanan/todo-front:latest: Status: Image is up to date for antonbanan/todo-front:latest"
Aug 12 20:53:59 minikube dockerd[1027]: time="2023-08-12T20:53:59.280313454Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9
Aug 12 20:53:59 minikube dockerd[1027]: time="2023-08-12T20:53:59.340616826Z" level=info msg="ignoring event" container=2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 12 20:53:59 minikube dockerd[1027]: time="2023-08-12T20:53:59.452664926Z" level=info msg="ignoring event" container=f6b77a96f997754146235e7ff38e8cdb4f17ab9a953a805514c3a80475c2a112 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 12 20:55:05 minikube cri-dockerd[1246]: time="2023-08-12T20:55:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/44f87ba82adb9c2536ffdcdca52d4fa219c946892504a060099a9a2a44b1d954/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 12 20:55:06 minikube cri-dockerd[1246]: time="2023-08-12T20:55:06Z" level=info msg="Stop pulling image docker.io/antonbanan/todo-front:latest: Status: Image is up to date for antonbanan/todo-front:latest"
Aug 12 21:00:40 minikube cri-dockerd[1246]: time="2023-08-12T21:00:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bd7b0ce95107ab7a286f44ebb9f3f88cb5c4a6edef8209a9f5177028db713ea4/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 12 21:00:40 minikube cri-dockerd[1246]: time="2023-08-12T21:00:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cf64a97c39daad63323da45feb04970cf14d40bb265d7c9ff09d92e65bbbe8f5/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 12 21:00:40 minikube dockerd[1027]: time="2023-08-12T21:00:40.979706824Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Aug 12 21:00:51 minikube cri-dockerd[1246]: time="2023-08-12T21:00:51Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Extracting [==================>                                ]  28.41MB/75.78MB"
Aug 12 21:00:56 minikube cri-dockerd[1246]: time="2023-08-12T21:00:56Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Aug 12 21:00:57 minikube dockerd[1027]: time="2023-08-12T21:00:57.074611600Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Aug 12 21:01:02 minikube cri-dockerd[1246]: time="2023-08-12T21:01:02Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
3571898b35b9b       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   10 minutes ago      Running             dashboard-metrics-scraper   0                   cf64a97c39daa       dashboard-metrics-scraper-5dd9cbfd69-q5nf9
276fd7a4af4a9       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         10 minutes ago      Running             kubernetes-dashboard        0                   bd7b0ce95107a       kubernetes-dashboard-5c5cfc8747-dpmmh
6b18f01b6297b       antonbanan/todo-front@sha256:2e40d932e526240d1b44203032a255a80f7ed890922aa028490b3869e13d9faf          16 minutes ago      Running             todo                        0                   44f87ba82adb9       todo
fed43e21320a4       mongo@sha256:7769474cddc634e5a90b078f437dabc8816b8a3900cb1710219d1179b805bb8e                          26 minutes ago      Running             yamlmongodb                 0                   9f154da29fea2       yamlmongodb
f596918e46891       antonbanan/todo-app@sha256:ceef2c1c47439efe9afa5b34ac68fd16869cff8b489c3b893f4e36fa38153637            27 minutes ago      Running             todo-app                    0                   c27ea6de8d3be       todo-app
c444673c8635e       6e38f40d628db                                                                                          27 minutes ago      Running             storage-provisioner         2                   b98517a7304ef       storage-provisioner
cca015af045f1       6e38f40d628db                                                                                          32 minutes ago      Exited              storage-provisioner         1                   b98517a7304ef       storage-provisioner
fa59093b162e4       ead0a4a53df89                                                                                          33 minutes ago      Running             coredns                     0                   2e6b82ae59f1b       coredns-5d78c9869d-k578m
702cb43536e9e       5780543258cf0                                                                                          33 minutes ago      Running             kube-proxy                  0                   cb3a401369abf       kube-proxy-56h26
05711870c1ceb       7cffc01dba0e1                                                                                          33 minutes ago      Running             kube-controller-manager     0                   68ae9e435cb69       kube-controller-manager-minikube
0a26ea9527566       08a0c939e61b7                                                                                          33 minutes ago      Running             kube-apiserver              0                   1d3e8a73d6f1e       kube-apiserver-minikube
68487163e0660       86b6af7dd652c                                                                                          33 minutes ago      Running             etcd                        0                   ca3211651bfa5       etcd-minikube
3cb1c7c9ec36d       41697ceeb70b3                                                                                          33 minutes ago      Running             kube-scheduler              0                   83055203456c6       kube-scheduler-minikube

* 
* ==> coredns [fa59093b162e] <==
* [INFO] 10.244.0.4:45552 - 23025 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000044025s
[INFO] 10.244.0.4:38778 - 39681 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000104864s
[INFO] 10.244.0.4:60659 - 27347 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000042265s
[INFO] 10.244.0.4:58686 - 61817 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000214612s
[INFO] 10.244.0.4:35630 - 7631 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000190194s
[INFO] 10.244.0.4:39874 - 15189 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000274324s
[INFO] 10.244.0.4:45474 - 33417 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000097534s
[INFO] 10.244.0.4:53186 - 38097 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000188728s
[INFO] 10.244.0.4:34328 - 35940 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000192585s
[INFO] 10.244.0.4:60293 - 32735 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000114985s
[INFO] 10.244.0.4:48138 - 3299 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000130983s
[INFO] 10.244.0.4:38302 - 5274 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000189457s
[INFO] 10.244.0.4:35181 - 47704 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.00011656s
[INFO] 10.244.0.4:56965 - 19056 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000118548s
[INFO] 10.244.0.4:38657 - 44665 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00011945s
[INFO] 10.244.0.4:40730 - 9030 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.00014341s
[INFO] 10.244.0.4:54174 - 47101 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000083109s
[INFO] 10.244.0.4:58350 - 34696 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000078129s
[INFO] 10.244.0.4:52792 - 25218 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000081061s
[INFO] 10.244.0.4:47170 - 2539 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000283152s
[INFO] 10.244.0.4:58749 - 12368 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000100355s
[INFO] 10.244.0.4:39493 - 9477 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001398s
[INFO] 10.244.0.4:54492 - 54354 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000135486s
[INFO] 10.244.0.4:46418 - 48586 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000225266s
[INFO] 10.244.0.4:37624 - 59295 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000161178s
[INFO] 10.244.0.4:43091 - 61640 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000155535s
[INFO] 10.244.0.4:60436 - 47298 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000180375s
[INFO] 10.244.0.4:35759 - 5134 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000146668s
[INFO] 10.244.0.4:54737 - 64451 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000132223s
[INFO] 10.244.0.4:56655 - 20248 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000073864s
[INFO] 10.244.0.4:60070 - 38042 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000073017s
[INFO] 10.244.0.4:34400 - 17566 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000176617s
[INFO] 10.244.0.4:48564 - 62510 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000084702s
[INFO] 10.244.0.4:58552 - 22382 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000135831s
[INFO] 10.244.0.4:43010 - 54867 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000081244s
[INFO] 10.244.0.4:44540 - 54772 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000189208s
[INFO] 10.244.0.4:52104 - 48538 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000615981s
[INFO] 10.244.0.4:59578 - 16652 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000375162s
[INFO] 10.244.0.4:57449 - 40967 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000097933s
[INFO] 10.244.0.4:55931 - 28516 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000196224s
[INFO] 10.244.0.4:56540 - 63908 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.00014102s
[INFO] 10.244.0.4:50811 - 27264 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000153552s
[INFO] 10.244.0.4:57078 - 36741 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000172294s
[INFO] 10.244.0.4:46591 - 22587 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000164272s
[INFO] 10.244.0.4:57430 - 20237 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000253893s
[INFO] 10.244.0.4:55829 - 40472 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000081496s
[INFO] 10.244.0.4:60533 - 38450 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000124353s
[INFO] 10.244.0.4:56670 - 42610 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.00033437s
[INFO] 10.244.0.4:33674 - 1622 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000159252s
[INFO] 10.244.0.4:33782 - 52258 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000194883s
[INFO] 10.244.0.4:41814 - 53935 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00010327s
[INFO] 10.244.0.4:39758 - 27803 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000116039s
[INFO] 10.244.0.4:47380 - 42146 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000077668s
[INFO] 10.244.0.4:56150 - 62779 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000161322s
[INFO] 10.244.0.4:55088 - 45127 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000091083s
[INFO] 10.244.0.4:47961 - 1428 "A IN yamlmongodb.default.svc.cluster.local.default.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000242803s
[INFO] 10.244.0.4:54148 - 48150 "A IN yamlmongodb.default.svc.cluster.local.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000208182s
[INFO] 10.244.0.4:53331 - 28409 "A IN yamlmongodb.default.svc.cluster.local.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000209835s
[INFO] 10.244.0.4:38583 - 12879 "A IN yamlmongodb.default.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00019172s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.114741122s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd3f3801765d093a485d255043149f92ec0a695f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_08_12T13_38_18_0700
                    minikube.k8s.io/version=v1.31.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 12 Aug 2023 20:38:15 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 12 Aug 2023 21:11:39 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 12 Aug 2023 21:11:30 +0000   Sat, 12 Aug 2023 20:38:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 12 Aug 2023 21:11:30 +0000   Sat, 12 Aug 2023 20:38:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 12 Aug 2023 21:11:30 +0000   Sat, 12 Aug 2023 20:38:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 12 Aug 2023 21:11:30 +0000   Sat, 12 Aug 2023 20:38:15 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8048088Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8048088Ki
  pods:               110
System Info:
  Machine ID:                 4051da58cff94e76bb61258b5d63c0ba
  System UUID:                7ba7fd76-c2af-4e0a-8ce6-247e21478f4c
  Boot ID:                    24fb75d7-a574-408f-af14-ad09c6c8888c
  Kernel Version:             5.15.49-linuxkit-pr
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.3
  Kube-Proxy Version:         v1.27.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     todo                                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16m
  default                     todo-app                                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28m
  default                     yamlmongodb                                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28m
  kube-system                 coredns-5d78c9869d-k578m                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     33m
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         33m
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         33m
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         33m
  kube-system                 kube-proxy-56h26                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         33m
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         33m
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         33m
  kubernetes-dashboard        dashboard-metrics-scraper-5dd9cbfd69-q5nf9    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kubernetes-dashboard        kubernetes-dashboard-5c5cfc8747-dpmmh         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 33m                kube-proxy       
  Normal  Starting                 33m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  33m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  33m (x8 over 33m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    33m (x8 over 33m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     33m (x7 over 33m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 33m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  33m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    33m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     33m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  33m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           33m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Aug12 19:35] ERROR: earlyprintk= earlyser already used
[  +0.000000] ERROR: earlyprintk= earlyser already used
[  +0.000000] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0x7E, should be 0xDB (20210730/tbprint-173)
[  +0.000000] ACPI: setting ELCR to 0200 (from 06e0)
[  +0.000000]  #2
[  +0.069442]  #3
[  +0.621132] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.040416] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.036261] ACPI Error: Could not enable RealTimeClock event (20210730/evxfevnt-182)
[  +0.002440] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20210730/evxface-618)
[  +0.011248] fail to initialize ptp_kvm
[  +0.000002] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +4.915854] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.004045] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +1.089411] grpcfuse: loading out-of-tree module taints kernel.
[Aug12 19:38] hrtimer: interrupt took 2285271 ns

* 
* ==> etcd [68487163e066] <==
* {"level":"info","ts":"2023-08-12T20:43:52.967Z","caller":"traceutil/trace.go:171","msg":"trace[1703046216] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:672; }","duration":"10.378299641s","start":"2023-08-12T20:43:42.589Z","end":"2023-08-12T20:43:52.967Z","steps":["trace[1703046216] 'agreement among raft nodes before linearized reading'  (duration: 10.378202224s)"],"step_count":1}
{"level":"warn","ts":"2023-08-12T20:43:52.967Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:42.589Z","time spent":"10.378344242s","remote":"127.0.0.1:37734","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":13,"response size":31,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true "}
{"level":"warn","ts":"2023-08-12T20:43:52.967Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"5.361648801s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-apiserver-minikube\" ","response":"range_response_count:1 size:7229"}
{"level":"info","ts":"2023-08-12T20:43:52.967Z","caller":"traceutil/trace.go:171","msg":"trace[1763029958] range","detail":"{range_begin:/registry/pods/kube-system/kube-apiserver-minikube; range_end:; response_count:1; response_revision:672; }","duration":"5.361696362s","start":"2023-08-12T20:43:47.606Z","end":"2023-08-12T20:43:52.967Z","steps":["trace[1763029958] 'agreement among raft nodes before linearized reading'  (duration: 5.361398027s)"],"step_count":1}
{"level":"warn","ts":"2023-08-12T20:43:52.967Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:47.606Z","time spent":"5.361791154s","remote":"127.0.0.1:37510","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":1,"response size":7253,"request content":"key:\"/registry/pods/kube-system/kube-apiserver-minikube\" "}
{"level":"warn","ts":"2023-08-12T20:43:52.967Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"6.37345213s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-apiserver-minikube.177abd8c215bc142\" ","response":"range_response_count:1 size:708"}
{"level":"info","ts":"2023-08-12T20:43:52.968Z","caller":"traceutil/trace.go:171","msg":"trace[1697268000] range","detail":"{range_begin:/registry/events/kube-system/kube-apiserver-minikube.177abd8c215bc142; range_end:; response_count:1; response_revision:672; }","duration":"6.374353638s","start":"2023-08-12T20:43:46.593Z","end":"2023-08-12T20:43:52.968Z","steps":["trace[1697268000] 'agreement among raft nodes before linearized reading'  (duration: 6.370077059s)"],"step_count":1}
{"level":"warn","ts":"2023-08-12T20:43:52.968Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:46.593Z","time spent":"6.374444726s","remote":"127.0.0.1:37424","response type":"/etcdserverpb.KV/Range","request count":0,"request size":71,"response count":1,"response size":732,"request content":"key:\"/registry/events/kube-system/kube-apiserver-minikube.177abd8c215bc142\" "}
{"level":"warn","ts":"2023-08-12T20:43:52.967Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.378740253s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas/default/\" range_end:\"/registry/resourcequotas/default0\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-12T20:43:52.968Z","caller":"traceutil/trace.go:171","msg":"trace[288819004] range","detail":"{range_begin:/registry/resourcequotas/default/; range_end:/registry/resourcequotas/default0; response_count:0; response_revision:672; }","duration":"3.379252195s","start":"2023-08-12T20:43:49.589Z","end":"2023-08-12T20:43:52.968Z","steps":["trace[288819004] 'agreement among raft nodes before linearized reading'  (duration: 3.378685969s)"],"step_count":1}
{"level":"warn","ts":"2023-08-12T20:43:52.968Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:49.589Z","time spent":"3.379382298s","remote":"127.0.0.1:37428","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":0,"response size":29,"request content":"key:\"/registry/resourcequotas/default/\" range_end:\"/registry/resourcequotas/default0\" "}
{"level":"warn","ts":"2023-08-12T20:43:52.967Z","caller":"etcdserver/server.go:1159","msg":"failed to revoke lease","lease-id":"70cc89eb78897944","error":"lease not found"}
{"level":"warn","ts":"2023-08-12T20:43:52.964Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:46.330Z","time spent":"6.634315039s","remote":"127.0.0.1:37426","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":0,"response size":29,"request content":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true "}
{"level":"warn","ts":"2023-08-12T20:43:52.990Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"155.933453ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-08-12T20:43:52.990Z","caller":"traceutil/trace.go:171","msg":"trace[37311719] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:672; }","duration":"156.020922ms","start":"2023-08-12T20:43:52.834Z","end":"2023-08-12T20:43:52.990Z","steps":["trace[37311719] 'agreement among raft nodes before linearized reading'  (duration: 135.14195ms)","trace[37311719] 'count revisions from in-memory index tree'  (duration: 20.753947ms)"],"step_count":2}
{"level":"warn","ts":"2023-08-12T20:43:52.990Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"913.493367ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-08-12T20:43:52.990Z","caller":"traceutil/trace.go:171","msg":"trace[1456250442] range","detail":"{range_begin:/registry/certificatesigningrequests/; range_end:/registry/certificatesigningrequests0; response_count:0; response_revision:672; }","duration":"913.643454ms","start":"2023-08-12T20:43:52.076Z","end":"2023-08-12T20:43:52.990Z","steps":["trace[1456250442] 'agreement among raft nodes before linearized reading'  (duration: 892.643335ms)","trace[1456250442] 'count revisions from in-memory index tree'  (duration: 20.803035ms)"],"step_count":2}
{"level":"warn","ts":"2023-08-12T20:43:52.990Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:52.076Z","time spent":"913.698963ms","remote":"127.0.0.1:37580","response type":"/etcdserverpb.KV/Range","request count":0,"request size":80,"response count":1,"response size":31,"request content":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true "}
{"level":"warn","ts":"2023-08-12T20:43:52.990Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.371882449s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-12T20:43:52.990Z","caller":"traceutil/trace.go:171","msg":"trace[1772504567] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:672; }","duration":"1.371931318s","start":"2023-08-12T20:43:51.618Z","end":"2023-08-12T20:43:52.990Z","steps":["trace[1772504567] 'agreement among raft nodes before linearized reading'  (duration: 1.350698858s)","trace[1772504567] 'range keys from in-memory index tree'  (duration: 21.145807ms)"],"step_count":2}
{"level":"warn","ts":"2023-08-12T20:43:52.990Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:51.618Z","time spent":"1.37198241s","remote":"127.0.0.1:37860","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-08-12T20:43:52.990Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"901.454487ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-12T20:43:52.990Z","caller":"traceutil/trace.go:171","msg":"trace[1955228183] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:672; }","duration":"901.524999ms","start":"2023-08-12T20:43:52.089Z","end":"2023-08-12T20:43:52.990Z","steps":["trace[1955228183] 'agreement among raft nodes before linearized reading'  (duration: 880.045143ms)","trace[1955228183] 'count revisions from in-memory index tree'  (duration: 21.393767ms)"],"step_count":2}
{"level":"warn","ts":"2023-08-12T20:43:52.991Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:52.089Z","time spent":"901.629765ms","remote":"127.0.0.1:37596","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":0,"response size":29,"request content":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true "}
{"level":"warn","ts":"2023-08-12T20:43:52.991Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.379098879s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-08-12T20:43:52.991Z","caller":"traceutil/trace.go:171","msg":"trace[1321776504] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/; range_end:/registry/apiregistration.k8s.io/apiservices0; response_count:0; response_revision:672; }","duration":"1.379149937s","start":"2023-08-12T20:43:51.611Z","end":"2023-08-12T20:43:52.991Z","steps":["trace[1321776504] 'agreement among raft nodes before linearized reading'  (duration: 1.3575526s)","trace[1321776504] 'count revisions from in-memory index tree'  (duration: 21.529622ms)"],"step_count":2}
{"level":"warn","ts":"2023-08-12T20:43:52.991Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:43:51.611Z","time spent":"1.379190116s","remote":"127.0.0.1:37844","response type":"/etcdserverpb.KV/Range","request count":0,"request size":96,"response count":21,"response size":31,"request content":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true "}
{"level":"info","ts":"2023-08-12T20:44:09.919Z","caller":"traceutil/trace.go:171","msg":"trace[131292254] transaction","detail":"{read_only:false; response_revision:688; number_of_response:1; }","duration":"166.313822ms","start":"2023-08-12T20:44:09.753Z","end":"2023-08-12T20:44:09.919Z","steps":["trace[131292254] 'process raft request'  (duration: 159.919144ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-12T20:44:24.416Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"180.592708ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/\" range_end:\"/registry/pods/default0\" limit:500 ","response":"range_response_count:3 size:7874"}
{"level":"info","ts":"2023-08-12T20:44:24.416Z","caller":"traceutil/trace.go:171","msg":"trace[1782158318] range","detail":"{range_begin:/registry/pods/default/; range_end:/registry/pods/default0; response_count:3; response_revision:699; }","duration":"180.656182ms","start":"2023-08-12T20:44:24.235Z","end":"2023-08-12T20:44:24.416Z","steps":["trace[1782158318] 'range keys from in-memory index tree'  (duration: 180.411957ms)"],"step_count":1}
{"level":"info","ts":"2023-08-12T20:45:04.210Z","caller":"traceutil/trace.go:171","msg":"trace[635240870] transaction","detail":"{read_only:false; response_revision:737; number_of_response:1; }","duration":"186.830084ms","start":"2023-08-12T20:45:04.023Z","end":"2023-08-12T20:45:04.210Z","steps":["trace[635240870] 'process raft request'  (duration: 186.721213ms)"],"step_count":1}
{"level":"info","ts":"2023-08-12T20:45:08.407Z","caller":"traceutil/trace.go:171","msg":"trace[310583699] linearizableReadLoop","detail":"{readStateIndex:836; appliedIndex:835; }","duration":"179.341624ms","start":"2023-08-12T20:45:08.227Z","end":"2023-08-12T20:45:08.407Z","steps":["trace[310583699] 'read index received'  (duration: 178.988411ms)","trace[310583699] 'applied index is now lower than readState.Index'  (duration: 351.901µs)"],"step_count":2}
{"level":"warn","ts":"2023-08-12T20:45:08.407Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"179.504654ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2023-08-12T20:45:08.407Z","caller":"traceutil/trace.go:171","msg":"trace[877659296] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:741; }","duration":"179.794681ms","start":"2023-08-12T20:45:08.227Z","end":"2023-08-12T20:45:08.407Z","steps":["trace[877659296] 'agreement among raft nodes before linearized reading'  (duration: 179.447548ms)"],"step_count":1}
{"level":"info","ts":"2023-08-12T20:48:13.736Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":647}
{"level":"info","ts":"2023-08-12T20:48:13.740Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":647,"took":"3.858303ms","hash":2219034251}
{"level":"info","ts":"2023-08-12T20:48:13.740Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2219034251,"revision":647,"compact-revision":-1}
{"level":"info","ts":"2023-08-12T20:49:18.806Z","caller":"traceutil/trace.go:171","msg":"trace[2113923532] transaction","detail":"{read_only:false; response_revision:945; number_of_response:1; }","duration":"925.982315ms","start":"2023-08-12T20:49:17.880Z","end":"2023-08-12T20:49:18.806Z","steps":["trace[2113923532] 'process raft request'  (duration: 925.612469ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-12T20:49:18.806Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T20:49:17.880Z","time spent":"926.254173ms","remote":"127.0.0.1:37484","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:944 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-08-12T20:53:05.626Z","caller":"traceutil/trace.go:171","msg":"trace[898162447] transaction","detail":"{read_only:false; response_revision:1143; number_of_response:1; }","duration":"106.454779ms","start":"2023-08-12T20:53:05.519Z","end":"2023-08-12T20:53:05.626Z","steps":["trace[898162447] 'process raft request'  (duration: 106.106644ms)"],"step_count":1}
{"level":"info","ts":"2023-08-12T20:53:13.686Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":895}
{"level":"warn","ts":"2023-08-12T20:53:13.700Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"129.088877ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128023071929500357 username:\"kube-apiserver-etcd-client\" auth_revision:1 > compaction:<revision:895 > ","response":"size:5"}
{"level":"info","ts":"2023-08-12T20:53:13.703Z","caller":"traceutil/trace.go:171","msg":"trace[1008908241] compact","detail":"{revision:895; response_revision:1148; }","duration":"162.136478ms","start":"2023-08-12T20:53:13.540Z","end":"2023-08-12T20:53:13.703Z","steps":["trace[1008908241] 'check and update compact revision'  (duration: 128.827181ms)"],"step_count":1}
{"level":"info","ts":"2023-08-12T20:53:13.707Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":895,"took":"19.037297ms","hash":3101098217}
{"level":"info","ts":"2023-08-12T20:53:13.708Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3101098217,"revision":895,"compact-revision":647}
{"level":"info","ts":"2023-08-12T20:58:13.491Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1148}
{"level":"info","ts":"2023-08-12T20:58:13.492Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":1148,"took":"822.85µs","hash":2673727131}
{"level":"info","ts":"2023-08-12T20:58:13.492Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2673727131,"revision":1148,"compact-revision":895}
{"level":"info","ts":"2023-08-12T21:03:13.280Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1415}
{"level":"info","ts":"2023-08-12T21:03:13.282Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":1415,"took":"794.591µs","hash":1436832236}
{"level":"info","ts":"2023-08-12T21:03:13.282Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1436832236,"revision":1415,"compact-revision":1148}
{"level":"warn","ts":"2023-08-12T21:07:54.844Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"208.206175ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-08-12T21:07:54.909Z","caller":"traceutil/trace.go:171","msg":"trace[1977386224] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:1951; }","duration":"327.818641ms","start":"2023-08-12T21:07:54.581Z","end":"2023-08-12T21:07:54.908Z","steps":["trace[1977386224] 'count revisions from in-memory index tree'  (duration: 205.403553ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-12T21:07:54.909Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-12T21:07:54.581Z","time spent":"328.594184ms","remote":"127.0.0.1:37500","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":1,"response size":31,"request content":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true "}
{"level":"info","ts":"2023-08-12T21:07:55.426Z","caller":"traceutil/trace.go:171","msg":"trace[707543913] transaction","detail":"{read_only:false; response_revision:1953; number_of_response:1; }","duration":"120.775545ms","start":"2023-08-12T21:07:55.305Z","end":"2023-08-12T21:07:55.426Z","steps":["trace[707543913] 'process raft request'  (duration: 120.604102ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-12T21:07:55.811Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.09145ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-08-12T21:07:55.812Z","caller":"traceutil/trace.go:171","msg":"trace[73117847] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1954; }","duration":"113.291767ms","start":"2023-08-12T21:07:55.698Z","end":"2023-08-12T21:07:55.811Z","steps":["trace[73117847] 'range keys from in-memory index tree'  (duration: 112.725082ms)"],"step_count":1}
{"level":"info","ts":"2023-08-12T21:08:13.049Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1732}
{"level":"info","ts":"2023-08-12T21:08:13.055Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":1732,"took":"4.834779ms","hash":3954978357}
{"level":"info","ts":"2023-08-12T21:08:13.055Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3954978357,"revision":1732,"compact-revision":1415}

* 
* ==> kernel <==
*  21:11:42 up  1:35,  0 users,  load average: 0.56, 0.90, 0.75
Linux minikube 5.15.49-linuxkit-pr #1 SMP Thu May 25 07:17:40 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [0a26ea952756] <==
* Trace[506879478]: ["Create etcd3" audit-id:4de6bec8-e4f2-4b97-af22-166e02ef66a5,key:/events/kube-system/kube-apiserver-minikube.177abd8c215bc142,type:*core.Event,resource:events 7003ms (20:43:39.586)
Trace[506879478]:  ---"Txn call failed" err:etcdserver: request timed out 7002ms (20:43:46.589)]
Trace[506879478]: [7.003937811s] [7.003937811s] END
E0812 20:43:48.819452       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:"etcdserver: request timed out"}: etcdserver: request timed out
I0812 20:43:48.819661       1 trace.go:219] Trace[1740633477]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:db6312f3-ea15-4f76-b95e-8197d2956498,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PUT (12-Aug-2023 20:43:41.815) (total time: 7003ms):
Trace[1740633477]: ["GuaranteedUpdate etcd3" audit-id:db6312f3-ea15-4f76-b95e-8197d2956498,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 7003ms (20:43:41.816)
Trace[1740633477]:  ---"Txn call failed" err:etcdserver: request timed out 7002ms (20:43:48.819)]
Trace[1740633477]: [7.003735448s] [7.003735448s] END
E0812 20:43:48.820453       1 controller.go:193] "Failed to update lease" err="etcdserver: request timed out"
E0812 20:43:49.584849       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:"etcdserver: request timed out"}: etcdserver: request timed out
I0812 20:43:49.585396       1 trace.go:219] Trace[202641474]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5c872e6b-eb60-4ebd-a4b4-6d42efef0ecc,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:resource,url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (12-Aug-2023 20:43:37.093) (total time: 12491ms):
Trace[202641474]: [12.4911106s] [12.4911106s] END
E0812 20:43:49.946964       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:"etcdserver: request timed out"}: etcdserver: request timed out
I0812 20:43:49.947122       1 trace.go:219] Trace[866544704]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:919e2363-9210-49bf-98fc-5b1fefc28be1,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PUT (12-Aug-2023 20:43:42.943) (total time: 7003ms):
Trace[866544704]: ["GuaranteedUpdate etcd3" audit-id:919e2363-9210-49bf-98fc-5b1fefc28be1,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 7003ms (20:43:42.943)
Trace[866544704]:  ---"Txn call failed" err:etcdserver: request timed out 7002ms (20:43:49.946)]
Trace[866544704]: [7.003694636s] [7.003694636s] END
I0812 20:43:52.966037       1 trace.go:219] Trace[1090757962]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:edb272ed-ed9b-495a-9bf1-8e6fe8b35340,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PUT (12-Aug-2023 20:43:48.821) (total time: 4144ms):
Trace[1090757962]: ["GuaranteedUpdate etcd3" audit-id:edb272ed-ed9b-495a-9bf1-8e6fe8b35340,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 4144ms (20:43:48.821)
Trace[1090757962]:  ---"Txn call completed" 4142ms (20:43:52.965)]
Trace[1090757962]: [4.14445807s] [4.14445807s] END
E0812 20:43:52.966457       1 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": the object has been modified; please apply your changes to the latest version and try again"
I0812 20:43:52.966494       1 trace.go:219] Trace[451552845]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:82cf672b-598b-45fc-894a-4809f483ed02,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PATCH (12-Aug-2023 20:43:47.025) (total time: 5940ms):
Trace[451552845]: ["GuaranteedUpdate etcd3" audit-id:82cf672b-598b-45fc-894a-4809f483ed02,key:/minions/minikube,type:*core.Node,resource:nodes 5940ms (20:43:47.026)
Trace[451552845]:  ---"Txn call completed" 5937ms (20:43:52.966)]
Trace[451552845]: ---"Object stored in database" 5938ms (20:43:52.966)
Trace[451552845]: [5.940796827s] [5.940796827s] END
I0812 20:43:52.969556       1 trace.go:219] Trace[817658214]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:56c1c505-ee4a-4bfd-8fa7-ce04ffe8f5f9,client:127.0.0.1,protocol:HTTP/2.0,resource:resourcequotas,scope:namespace,url:/api/v1/namespaces/default/resourcequotas,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:LIST (12-Aug-2023 20:43:49.588) (total time: 3381ms):
Trace[817658214]: ["List(recursive=true) etcd3" audit-id:56c1c505-ee4a-4bfd-8fa7-ce04ffe8f5f9,key:/resourcequotas/default,resourceVersion:,resourceVersionMatch:,limit:0,continue: 3381ms (20:43:49.588)]
Trace[817658214]: [3.381143644s] [3.381143644s] END
I0812 20:43:52.969584       1 trace.go:219] Trace[1309940505]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b2eab544-cf27-40d5-b1c2-83f68d8898f1,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PUT (12-Aug-2023 20:43:49.949) (total time: 3019ms):
Trace[1309940505]: ["GuaranteedUpdate etcd3" audit-id:b2eab544-cf27-40d5-b1c2-83f68d8898f1,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 3019ms (20:43:49.949)
Trace[1309940505]:  ---"Txn call completed" 3018ms (20:43:52.969)]
Trace[1309940505]: [3.019721393s] [3.019721393s] END
I0812 20:43:52.972193       1 trace.go:219] Trace[855008307]: "List" accept:application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json,audit-id:29dce544-b71d-46ff-b109-0690d3a4ba3f,client:192.168.49.1,protocol:HTTP/2.0,resource:pods,scope:namespace,url:/api/v1/namespaces/default/pods,user-agent:kubectl/v1.27.2 (darwin/amd64) kubernetes/7f6f68f,verb:LIST (12-Aug-2023 20:43:44.889) (total time: 8083ms):
Trace[855008307]: ["List(recursive=true) etcd3" audit-id:29dce544-b71d-46ff-b109-0690d3a4ba3f,key:/pods/default,resourceVersion:,resourceVersionMatch:,limit:500,continue: 8082ms (20:43:44.889)]
Trace[855008307]: [8.083071361s] [8.083071361s] END
I0812 20:43:52.972940       1 trace.go:219] Trace[994479399]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f0bbba12-d4a1-4acd-9dae-dca5e2fd7ecc,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (12-Aug-2023 20:43:47.605) (total time: 5367ms):
Trace[994479399]: ---"About to write a response" 5366ms (20:43:52.971)
Trace[994479399]: [5.367434206s] [5.367434206s] END
I0812 20:43:52.992494       1 trace.go:219] Trace[1444918517]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b24c5679-d2c3-4d01-ae94-d908a0ea4184,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.177abd8c215bc142,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PATCH (12-Aug-2023 20:43:46.592) (total time: 6399ms):
Trace[1444918517]: ["GuaranteedUpdate etcd3" audit-id:b24c5679-d2c3-4d01-ae94-d908a0ea4184,key:/events/kube-system/kube-apiserver-minikube.177abd8c215bc142,type:*core.Event,resource:events 6399ms (20:43:46.593)
Trace[1444918517]:  ---"initial value restored" 6376ms (20:43:52.970)]
Trace[1444918517]: ---"Object stored in database" 17ms (20:43:52.992)
Trace[1444918517]: [6.399306085s] [6.399306085s] END
I0812 20:43:53.000052       1 trace.go:219] Trace[1243806349]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:dffc6f41-5006-4eda-94f0-85d4e7aaffaf,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:resource,url:/api/v1/namespaces,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:POST (12-Aug-2023 20:43:49.587) (total time: 3412ms):
Trace[1243806349]: [3.412706596s] [3.412706596s] END
I0812 20:49:18.820396       1 trace.go:219] Trace[1596294070]: "Update" accept:application/json, */*,audit-id:974679d8-28a8-4da2-9552-e274c2071296,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (12-Aug-2023 20:49:17.626) (total time: 1185ms):
Trace[1596294070]: ["GuaranteedUpdate etcd3" audit-id:974679d8-28a8-4da2-9552-e274c2071296,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1182ms (20:49:17.629)
Trace[1596294070]:  ---"Txn call completed" 1161ms (20:49:18.811)]
Trace[1596294070]: [1.185641953s] [1.185641953s] END
I0812 20:55:10.454803       1 alloc.go:330] "allocated clusterIPs" service="default/todo" clusterIPs=map[IPv4:10.108.95.150]
I0812 21:00:39.937453       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs=map[IPv4:10.107.138.70]
I0812 21:00:39.960366       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs=map[IPv4:10.104.130.121]
I0812 21:07:55.449785       1 trace.go:219] Trace[410541805]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8ad0913f-5db5-42ab-826e-cd32a59947bb,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PUT (12-Aug-2023 21:07:54.783) (total time: 664ms):
Trace[410541805]: ---"About to convert to expected version" 26ms (21:07:54.813)
Trace[410541805]: ["GuaranteedUpdate etcd3" audit-id:8ad0913f-5db5-42ab-826e-cd32a59947bb,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 633ms (21:07:54.814)
Trace[410541805]:  ---"About to Encode" 440ms (21:07:55.257)
Trace[410541805]:  ---"Txn call completed" 185ms (21:07:55.443)]
Trace[410541805]: [664.670398ms] [664.670398ms] END

* 
* ==> kube-controller-manager [05711870c1ce] <==
* I0812 20:38:30.717929       1 shared_informer.go:318] Caches are synced for attach detach
I0812 20:38:30.718226       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0812 20:38:30.725687       1 shared_informer.go:318] Caches are synced for crt configmap
I0812 20:38:30.725951       1 shared_informer.go:318] Caches are synced for GC
I0812 20:38:30.733005       1 shared_informer.go:318] Caches are synced for deployment
I0812 20:38:30.734365       1 shared_informer.go:318] Caches are synced for ephemeral
I0812 20:38:30.737233       1 shared_informer.go:318] Caches are synced for stateful set
I0812 20:38:30.737741       1 shared_informer.go:318] Caches are synced for taint
I0812 20:38:30.738041       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0812 20:38:30.738140       1 shared_informer.go:318] Caches are synced for expand
I0812 20:38:30.738160       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0812 20:38:30.738395       1 taint_manager.go:211] "Sending events to api server"
I0812 20:38:30.738602       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0812 20:38:30.738614       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0812 20:38:30.738817       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0812 20:38:30.741450       1 shared_informer.go:318] Caches are synced for TTL
I0812 20:38:30.741856       1 shared_informer.go:318] Caches are synced for ReplicationController
I0812 20:38:30.742412       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0812 20:38:30.743664       1 shared_informer.go:318] Caches are synced for PV protection
I0812 20:38:30.764951       1 shared_informer.go:318] Caches are synced for persistent volume
I0812 20:38:30.770483       1 shared_informer.go:318] Caches are synced for cronjob
I0812 20:38:30.772952       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0812 20:38:30.778368       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0812 20:38:30.786415       1 shared_informer.go:318] Caches are synced for TTL after finished
I0812 20:38:30.786652       1 shared_informer.go:318] Caches are synced for HPA
I0812 20:38:30.788074       1 shared_informer.go:318] Caches are synced for PVC protection
I0812 20:38:30.788210       1 shared_informer.go:318] Caches are synced for daemon sets
I0812 20:38:30.788157       1 shared_informer.go:318] Caches are synced for disruption
I0812 20:38:30.789661       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0812 20:38:30.789744       1 shared_informer.go:318] Caches are synced for service account
I0812 20:38:30.789865       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0812 20:38:30.791345       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0812 20:38:30.791470       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0812 20:38:30.791566       1 shared_informer.go:318] Caches are synced for job
I0812 20:38:30.791581       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0812 20:38:30.792755       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0812 20:38:30.792780       1 shared_informer.go:318] Caches are synced for namespace
I0812 20:38:30.803536       1 shared_informer.go:318] Caches are synced for node
I0812 20:38:30.803808       1 range_allocator.go:174] "Sending events to api server"
I0812 20:38:30.804294       1 range_allocator.go:178] "Starting range CIDR allocator"
I0812 20:38:30.804469       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0812 20:38:30.804483       1 shared_informer.go:318] Caches are synced for cidrallocator
I0812 20:38:30.814163       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=[10.244.0.0/24]
I0812 20:38:30.843190       1 shared_informer.go:318] Caches are synced for endpoint
I0812 20:38:30.947403       1 shared_informer.go:318] Caches are synced for resource quota
I0812 20:38:30.993665       1 shared_informer.go:318] Caches are synced for resource quota
I0812 20:38:31.309449       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5d78c9869d to 1"
I0812 20:38:31.310918       1 shared_informer.go:318] Caches are synced for garbage collector
I0812 20:38:31.357332       1 shared_informer.go:318] Caches are synced for garbage collector
I0812 20:38:31.357479       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0812 20:38:31.651530       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-56h26"
I0812 20:38:31.795132       1 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5d78c9869d-k578m"
I0812 21:00:39.719431       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set kubernetes-dashboard-5c5cfc8747 to 1"
I0812 21:00:39.719631       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dashboard-metrics-scraper-5dd9cbfd69 to 1"
I0812 21:00:39.732680       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-5c5cfc8747-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0812 21:00:39.732743       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-5dd9cbfd69-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0812 21:00:39.753030       1 replica_set.go:544] sync "kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" failed with pods "kubernetes-dashboard-5c5cfc8747-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
E0812 21:00:39.757627       1 replica_set.go:544] sync "kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" failed with pods "dashboard-metrics-scraper-5dd9cbfd69-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0812 21:00:39.813748       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-5dd9cbfd69-q5nf9"
I0812 21:00:39.814995       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-5c5cfc8747-dpmmh"

* 
* ==> kube-proxy [702cb43536e9] <==
* I0812 20:38:32.319153       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0812 20:38:32.319298       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0812 20:38:32.319315       1 server_others.go:554] "Using iptables proxy"
I0812 20:38:32.372419       1 server_others.go:192] "Using iptables Proxier"
I0812 20:38:32.372555       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0812 20:38:32.372571       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0812 20:38:32.372593       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0812 20:38:32.372637       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0812 20:38:32.373872       1 server.go:658] "Version info" version="v1.27.3"
I0812 20:38:32.373922       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0812 20:38:32.374836       1 config.go:315] "Starting node config controller"
I0812 20:38:32.374881       1 shared_informer.go:311] Waiting for caches to sync for node config
I0812 20:38:32.374935       1 config.go:97] "Starting endpoint slice config controller"
I0812 20:38:32.374942       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0812 20:38:32.375011       1 config.go:188] "Starting service config controller"
I0812 20:38:32.375017       1 shared_informer.go:311] Waiting for caches to sync for service config
I0812 20:38:32.475157       1 shared_informer.go:318] Caches are synced for service config
I0812 20:38:32.475204       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0812 20:38:32.475226       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [3cb1c7c9ec36] <==
* I0812 20:38:13.213587       1 serving.go:348] Generated self-signed cert in-memory
W0812 20:38:15.813153       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0812 20:38:15.813250       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0812 20:38:15.813262       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0812 20:38:15.813268       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0812 20:38:15.828738       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.3"
I0812 20:38:15.828830       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0812 20:38:15.831686       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0812 20:38:15.831998       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0812 20:38:15.832065       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0812 20:38:15.832451       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0812 20:38:15.836520       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0812 20:38:15.836626       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0812 20:38:15.898740       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0812 20:38:15.898819       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0812 20:38:15.898931       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0812 20:38:15.899045       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0812 20:38:15.901068       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0812 20:38:15.901148       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0812 20:38:15.901155       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0812 20:38:15.901262       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0812 20:38:15.902987       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0812 20:38:15.906564       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0812 20:38:15.903699       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0812 20:38:15.906598       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0812 20:38:15.903902       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0812 20:38:15.906665       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0812 20:38:15.903981       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0812 20:38:15.906926       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0812 20:38:15.904003       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0812 20:38:15.907127       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0812 20:38:15.904120       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0812 20:38:15.907356       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0812 20:38:15.907375       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0812 20:38:15.907432       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0812 20:38:15.904549       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0812 20:38:15.907734       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0812 20:38:15.906311       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0812 20:38:15.907782       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0812 20:38:15.906408       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0812 20:38:15.907801       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0812 20:38:16.723089       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0812 20:38:16.723188       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0812 20:38:16.731040       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0812 20:38:16.731231       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0812 20:38:16.770594       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0812 20:38:16.770722       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0812 20:38:16.868007       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0812 20:38:16.868035       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0812 20:38:16.989102       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0812 20:38:16.989196       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0812 20:38:17.028738       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0812 20:38:17.028791       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
I0812 20:38:19.733192       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Aug 12 20:43:02 minikube kubelet[2306]: I0812 20:43:02.363989    2306 topology_manager.go:212] "Topology Admit Handler"
Aug 12 20:43:02 minikube kubelet[2306]: I0812 20:43:02.559516    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-82dwf\" (UniqueName: \"kubernetes.io/projected/052f9766-22e9-45d9-8064-5301c796040a-kube-api-access-82dwf\") pod \"todo-front\" (UID: \"052f9766-22e9-45d9-8064-5301c796040a\") " pod="default/todo-front"
Aug 12 20:43:02 minikube kubelet[2306]: I0812 20:43:02.605936    2306 topology_manager.go:212] "Topology Admit Handler"
Aug 12 20:43:02 minikube kubelet[2306]: I0812 20:43:02.660745    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ks889\" (UniqueName: \"kubernetes.io/projected/edf23762-4fdc-4899-93e9-e3e5df13c975-kube-api-access-ks889\") pod \"todo-app\" (UID: \"edf23762-4fdc-4899-93e9-e3e5df13c975\") " pod="default/todo-app"
Aug 12 20:43:02 minikube kubelet[2306]: I0812 20:43:02.861591    2306 topology_manager.go:212] "Topology Admit Handler"
Aug 12 20:43:02 minikube kubelet[2306]: I0812 20:43:02.965162    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-98cpw\" (UniqueName: \"kubernetes.io/projected/94f67164-bb95-4082-a3a8-03c6a570fd4f-kube-api-access-98cpw\") pod \"yamlmongodb\" (UID: \"94f67164-bb95-4082-a3a8-03c6a570fd4f\") " pod="default/yamlmongodb"
Aug 12 20:43:03 minikube kubelet[2306]: I0812 20:43:03.397481    2306 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9f154da29fea256d64c6026b485efa831e820a3d95bb3f90f0b3ef3a41794c26"
Aug 12 20:43:18 minikube kubelet[2306]: W0812 20:43:18.788285    2306 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 12 20:43:42 minikube kubelet[2306]: E0812 20:43:42.941626    2306 controller.go:193] "Failed to update lease" err="etcdserver: request timed out"
Aug 12 20:43:46 minikube kubelet[2306]: E0812 20:43:46.590976    2306 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-apiserver-minikube.177abd8c215bc142", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-apiserver-minikube", UID:"4e275e35949ad3fdfeb753c1099308e7", APIVersion:"v1", ResourceVersion:"", FieldPath:"spec.containers{kube-apiserver}"}, Reason:"Unhealthy", Message:"Readiness probe failed: HTTP probe failed with statuscode: 500", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:time.Date(2023, time.August, 12, 20, 43, 39, 583775042, time.Local), LastTimestamp:time.Date(2023, time.August, 12, 20, 43, 39, 583775042, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'etcdserver: request timed out' (will not retry!)
Aug 12 20:43:49 minikube kubelet[2306]: E0812 20:43:49.948314    2306 controller.go:193] "Failed to update lease" err="etcdserver: request timed out"
Aug 12 20:43:52 minikube kubelet[2306]: E0812 20:43:52.970104    2306 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"minikube\": the object has been modified; please apply your changes to the latest version and try again"
Aug 12 20:43:53 minikube kubelet[2306]: I0812 20:43:53.903694    2306 scope.go:115] "RemoveContainer" containerID="622196d76c398fc4b1d30d2b02b4e6509526d698495b150bfd144b65f11dc019"
Aug 12 20:43:53 minikube kubelet[2306]: I0812 20:43:53.904500    2306 scope.go:115] "RemoveContainer" containerID="cca015af045f1960e57483a9821357f3c943d8f0b4251194441ddcac6393185f"
Aug 12 20:43:53 minikube kubelet[2306]: E0812 20:43:53.904893    2306 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(6e96bd61-c362-4709-b1e1-9ac79e398bae)\"" pod="kube-system/storage-provisioner" podUID=6e96bd61-c362-4709-b1e1-9ac79e398bae
Aug 12 20:44:09 minikube kubelet[2306]: I0812 20:44:09.744370    2306 scope.go:115] "RemoveContainer" containerID="cca015af045f1960e57483a9821357f3c943d8f0b4251194441ddcac6393185f"
Aug 12 20:44:33 minikube kubelet[2306]: I0812 20:44:33.204075    2306 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/todo-front" podStartSLOduration=10.325825889 podCreationTimestamp="2023-08-12 20:43:02 +0000 UTC" firstStartedPulling="2023-08-12 20:43:03.017641067 +0000 UTC m=+284.564425821" lastFinishedPulling="2023-08-12 20:44:23.852162072 +0000 UTC m=+365.442632351" observedRunningTime="2023-08-12 20:44:25.152197111 +0000 UTC m=+366.742667378" watchObservedRunningTime="2023-08-12 20:44:33.204032419 +0000 UTC m=+374.816561405"
Aug 12 20:45:08 minikube kubelet[2306]: I0812 20:45:08.694892    2306 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/todo-app" podStartSLOduration=37.545981305 podCreationTimestamp="2023-08-12 20:43:02 +0000 UTC" firstStartedPulling="2023-08-12 20:43:03.196123515 +0000 UTC m=+284.742908269" lastFinishedPulling="2023-08-12 20:44:32.279250188 +0000 UTC m=+373.891779171" observedRunningTime="2023-08-12 20:44:33.204465174 +0000 UTC m=+374.816994174" watchObservedRunningTime="2023-08-12 20:45:08.694852207 +0000 UTC m=+410.329186169"
Aug 12 20:48:18 minikube kubelet[2306]: W0812 20:48:18.582212    2306 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 12 20:49:18 minikube kubelet[2306]: E0812 20:49:18.815877    2306 kubelet.go:2431] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.287s"
Aug 12 20:50:40 minikube kubelet[2306]: I0812 20:50:40.854621    2306 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/yamlmongodb" podStartSLOduration=334.687215462 podCreationTimestamp="2023-08-12 20:43:02 +0000 UTC" firstStartedPulling="2023-08-12 20:43:03.451414176 +0000 UTC m=+284.998198930" lastFinishedPulling="2023-08-12 20:45:07.523404875 +0000 UTC m=+409.157738831" observedRunningTime="2023-08-12 20:45:08.695417408 +0000 UTC m=+410.329751376" watchObservedRunningTime="2023-08-12 20:50:40.846755363 +0000 UTC m=+742.723374685"
Aug 12 20:51:11 minikube kubelet[2306]: I0812 20:51:11.211917    2306 scope.go:115] "RemoveContainer" containerID="94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f"
Aug 12 20:51:11 minikube kubelet[2306]: I0812 20:51:11.235842    2306 scope.go:115] "RemoveContainer" containerID="94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f"
Aug 12 20:51:11 minikube kubelet[2306]: E0812 20:51:11.236980    2306 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f" containerID="94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f"
Aug 12 20:51:11 minikube kubelet[2306]: I0812 20:51:11.237055    2306 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f} err="failed to get container status \"94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f\": rpc error: code = Unknown desc = Error response from daemon: No such container: 94a5e5a64e3331a8b7815b3d4ea66a12f8f5e82e58d08cc422512311507f134f"
Aug 12 20:51:11 minikube kubelet[2306]: I0812 20:51:11.327852    2306 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-82dwf\" (UniqueName: \"kubernetes.io/projected/052f9766-22e9-45d9-8064-5301c796040a-kube-api-access-82dwf\") pod \"052f9766-22e9-45d9-8064-5301c796040a\" (UID: \"052f9766-22e9-45d9-8064-5301c796040a\") "
Aug 12 20:51:11 minikube kubelet[2306]: I0812 20:51:11.349113    2306 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/052f9766-22e9-45d9-8064-5301c796040a-kube-api-access-82dwf" (OuterVolumeSpecName: "kube-api-access-82dwf") pod "052f9766-22e9-45d9-8064-5301c796040a" (UID: "052f9766-22e9-45d9-8064-5301c796040a"). InnerVolumeSpecName "kube-api-access-82dwf". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 12 20:51:11 minikube kubelet[2306]: I0812 20:51:11.430134    2306 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-82dwf\" (UniqueName: \"kubernetes.io/projected/052f9766-22e9-45d9-8064-5301c796040a-kube-api-access-82dwf\") on node \"minikube\" DevicePath \"\""
Aug 12 20:51:12 minikube kubelet[2306]: I0812 20:51:12.447833    2306 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=052f9766-22e9-45d9-8064-5301c796040a path="/var/lib/kubelet/pods/052f9766-22e9-45d9-8064-5301c796040a/volumes"
Aug 12 20:51:22 minikube kubelet[2306]: I0812 20:51:22.739562    2306 topology_manager.go:212] "Topology Admit Handler"
Aug 12 20:51:22 minikube kubelet[2306]: E0812 20:51:22.743601    2306 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="052f9766-22e9-45d9-8064-5301c796040a" containerName="todo-front"
Aug 12 20:51:22 minikube kubelet[2306]: I0812 20:51:22.744557    2306 memory_manager.go:346] "RemoveStaleState removing state" podUID="052f9766-22e9-45d9-8064-5301c796040a" containerName="todo-front"
Aug 12 20:51:22 minikube kubelet[2306]: I0812 20:51:22.946816    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ghxrf\" (UniqueName: \"kubernetes.io/projected/61adf90b-9107-4eb6-b312-d0bb473c22db-kube-api-access-ghxrf\") pod \"todo-front\" (UID: \"61adf90b-9107-4eb6-b312-d0bb473c22db\") " pod="default/todo-front"
Aug 12 20:53:18 minikube kubelet[2306]: W0812 20:53:18.363614    2306 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 12 20:53:29 minikube kubelet[2306]: I0812 20:53:29.268311    2306 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/todo-front" podStartSLOduration=126.165576684 podCreationTimestamp="2023-08-12 20:51:22 +0000 UTC" firstStartedPulling="2023-08-12 20:51:23.629871756 +0000 UTC m=+785.528483232" lastFinishedPulling="2023-08-12 20:51:24.731972057 +0000 UTC m=+786.630583534" observedRunningTime="2023-08-12 20:51:25.404764456 +0000 UTC m=+787.303375939" watchObservedRunningTime="2023-08-12 20:53:29.267676986 +0000 UTC m=+911.275330318"
Aug 12 20:53:59 minikube kubelet[2306]: I0812 20:53:59.609822    2306 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-ghxrf\" (UniqueName: \"kubernetes.io/projected/61adf90b-9107-4eb6-b312-d0bb473c22db-kube-api-access-ghxrf\") pod \"61adf90b-9107-4eb6-b312-d0bb473c22db\" (UID: \"61adf90b-9107-4eb6-b312-d0bb473c22db\") "
Aug 12 20:53:59 minikube kubelet[2306]: I0812 20:53:59.614861    2306 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/61adf90b-9107-4eb6-b312-d0bb473c22db-kube-api-access-ghxrf" (OuterVolumeSpecName: "kube-api-access-ghxrf") pod "61adf90b-9107-4eb6-b312-d0bb473c22db" (UID: "61adf90b-9107-4eb6-b312-d0bb473c22db"). InnerVolumeSpecName "kube-api-access-ghxrf". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 12 20:53:59 minikube kubelet[2306]: I0812 20:53:59.711565    2306 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-ghxrf\" (UniqueName: \"kubernetes.io/projected/61adf90b-9107-4eb6-b312-d0bb473c22db-kube-api-access-ghxrf\") on node \"minikube\" DevicePath \"\""
Aug 12 20:54:00 minikube kubelet[2306]: I0812 20:54:00.164305    2306 scope.go:115] "RemoveContainer" containerID="2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9"
Aug 12 20:54:00 minikube kubelet[2306]: I0812 20:54:00.191746    2306 scope.go:115] "RemoveContainer" containerID="2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9"
Aug 12 20:54:00 minikube kubelet[2306]: E0812 20:54:00.194063    2306 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9" containerID="2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9"
Aug 12 20:54:00 minikube kubelet[2306]: I0812 20:54:00.194273    2306 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9} err="failed to get container status \"2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9\": rpc error: code = Unknown desc = Error response from daemon: No such container: 2612d04289f860404773e357999f96aa17bd54bb7a36efd82bdf2f18472769d9"
Aug 12 20:54:00 minikube kubelet[2306]: I0812 20:54:00.317610    2306 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=61adf90b-9107-4eb6-b312-d0bb473c22db path="/var/lib/kubelet/pods/61adf90b-9107-4eb6-b312-d0bb473c22db/volumes"
Aug 12 20:55:05 minikube kubelet[2306]: I0812 20:55:05.294069    2306 topology_manager.go:212] "Topology Admit Handler"
Aug 12 20:55:05 minikube kubelet[2306]: E0812 20:55:05.294260    2306 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="61adf90b-9107-4eb6-b312-d0bb473c22db" containerName="todo-front"
Aug 12 20:55:05 minikube kubelet[2306]: I0812 20:55:05.294319    2306 memory_manager.go:346] "RemoveStaleState removing state" podUID="61adf90b-9107-4eb6-b312-d0bb473c22db" containerName="todo-front"
Aug 12 20:55:05 minikube kubelet[2306]: I0812 20:55:05.468102    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w5mvp\" (UniqueName: \"kubernetes.io/projected/a25ee581-d3d2-491a-9dff-11a13f18a53a-kube-api-access-w5mvp\") pod \"todo\" (UID: \"a25ee581-d3d2-491a-9dff-11a13f18a53a\") " pod="default/todo"
Aug 12 20:55:05 minikube kubelet[2306]: I0812 20:55:05.837636    2306 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="44f87ba82adb9c2536ffdcdca52d4fa219c946892504a060099a9a2a44b1d954"
Aug 12 20:58:18 minikube kubelet[2306]: W0812 20:58:18.133768    2306 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 12 21:00:39 minikube kubelet[2306]: I0812 21:00:39.830358    2306 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/todo" podStartSLOduration=333.838287797 podCreationTimestamp="2023-08-12 20:55:05 +0000 UTC" firstStartedPulling="2023-08-12 20:55:05.885290145 +0000 UTC m=+1007.958804216" lastFinishedPulling="2023-08-12 20:55:06.877284074 +0000 UTC m=+1008.950798142" observedRunningTime="2023-08-12 20:55:07.892263463 +0000 UTC m=+1009.965777544" watchObservedRunningTime="2023-08-12 21:00:39.830281723 +0000 UTC m=+1342.145185009"
Aug 12 21:00:39 minikube kubelet[2306]: I0812 21:00:39.830708    2306 topology_manager.go:212] "Topology Admit Handler"
Aug 12 21:00:39 minikube kubelet[2306]: I0812 21:00:39.837219    2306 topology_manager.go:212] "Topology Admit Handler"
Aug 12 21:00:39 minikube kubelet[2306]: I0812 21:00:39.864047    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-grzbd\" (UniqueName: \"kubernetes.io/projected/179afc45-f97a-4d0f-927c-c6ed1191531e-kube-api-access-grzbd\") pod \"dashboard-metrics-scraper-5dd9cbfd69-q5nf9\" (UID: \"179afc45-f97a-4d0f-927c-c6ed1191531e\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-q5nf9"
Aug 12 21:00:39 minikube kubelet[2306]: I0812 21:00:39.864219    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/179afc45-f97a-4d0f-927c-c6ed1191531e-tmp-volume\") pod \"dashboard-metrics-scraper-5dd9cbfd69-q5nf9\" (UID: \"179afc45-f97a-4d0f-927c-c6ed1191531e\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-q5nf9"
Aug 12 21:00:39 minikube kubelet[2306]: I0812 21:00:39.864312    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/67008831-9c27-4ba7-9734-3eda9faae77b-tmp-volume\") pod \"kubernetes-dashboard-5c5cfc8747-dpmmh\" (UID: \"67008831-9c27-4ba7-9734-3eda9faae77b\") " pod="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-dpmmh"
Aug 12 21:00:39 minikube kubelet[2306]: I0812 21:00:39.864506    2306 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-glg26\" (UniqueName: \"kubernetes.io/projected/67008831-9c27-4ba7-9734-3eda9faae77b-kube-api-access-glg26\") pod \"kubernetes-dashboard-5c5cfc8747-dpmmh\" (UID: \"67008831-9c27-4ba7-9734-3eda9faae77b\") " pod="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-dpmmh"
Aug 12 21:01:03 minikube kubelet[2306]: I0812 21:01:03.470326    2306 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-dpmmh" podStartSLOduration=8.307985651 podCreationTimestamp="2023-08-12 21:00:39 +0000 UTC" firstStartedPulling="2023-08-12 21:00:40.633745612 +0000 UTC m=+1342.948648882" lastFinishedPulling="2023-08-12 21:00:56.773574392 +0000 UTC m=+1359.110931513" observedRunningTime="2023-08-12 21:00:58.069880131 +0000 UTC m=+1360.407237271" watchObservedRunningTime="2023-08-12 21:01:03.470268282 +0000 UTC m=+1365.807625409"
Aug 12 21:01:03 minikube kubelet[2306]: I0812 21:01:03.470424    2306 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-q5nf9" podStartSLOduration=2.22412612 podCreationTimestamp="2023-08-12 21:00:39 +0000 UTC" firstStartedPulling="2023-08-12 21:00:40.637186299 +0000 UTC m=+1342.952089560" lastFinishedPulling="2023-08-12 21:01:02.86101225 +0000 UTC m=+1365.198369388" observedRunningTime="2023-08-12 21:01:03.469930073 +0000 UTC m=+1365.807287206" watchObservedRunningTime="2023-08-12 21:01:03.470405948 +0000 UTC m=+1365.807763084"
Aug 12 21:03:17 minikube kubelet[2306]: W0812 21:03:17.911171    2306 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 12 21:08:17 minikube kubelet[2306]: W0812 21:08:17.679970    2306 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> kubernetes-dashboard [276fd7a4af4a] <==
* 2023/08/12 21:02:17 [2023-08-12T21:02:17Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:17 [2023-08-12T21:02:17Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:17 Getting list of all services in the cluster
2023/08/12 21:02:17 [2023-08-12T21:02:17Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:22 [2023-08-12T21:02:22Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/12 21:02:22 Getting list of namespaces
2023/08/12 21:02:22 [2023-08-12T21:02:22Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:22 [2023-08-12T21:02:22Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:22 Getting list of all services in the cluster
2023/08/12 21:02:22 [2023-08-12T21:02:22Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/12 21:02:27 Getting list of namespaces
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:27 Getting list of all services in the cluster
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/pod?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/ingress?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo request from 127.0.0.1: 
2023/08/12 21:02:27 Getting details of todo service in default namespace
2023/08/12 21:02:27 Found 5 events related to todo service in default namespace
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:27 Found 0 endpoints related to todo service in default namespace
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:27 Getting pod metrics
2023/08/12 21:02:27 [2023-08-12T21:02:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/12 21:02:32 Getting list of namespaces
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo request from 127.0.0.1: 
2023/08/12 21:02:32 Getting details of todo service in default namespace
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/pod?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/ingress?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:32 Found 5 events related to todo service in default namespace
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:32 Found 0 endpoints related to todo service in default namespace
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:32 Getting pod metrics
2023/08/12 21:02:32 [2023-08-12T21:02:32Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/08/12 21:02:36 Getting list of namespaces
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo request from 127.0.0.1: 
2023/08/12 21:02:36 Getting details of todo service in default namespace
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/pod?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Incoming HTTP/1.1 GET /api/v1/service/default/todo/ingress?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:36 Found 5 events related to todo service in default namespace
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:36 Found 0 endpoints related to todo service in default namespace
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Outcoming response to 127.0.0.1 with 200 status code
2023/08/12 21:02:36 Getting pod metrics
2023/08/12 21:02:36 [2023-08-12T21:02:36Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [c444673c8635] <==
* I0812 20:44:10.388799       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0812 20:44:10.428257       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0812 20:44:10.428342       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0812 20:44:27.814413       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0812 20:44:27.814594       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c5618fb0-df0c-4452-9946-42f4df31e5db!
I0812 20:44:27.815621       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ce9cb79b-ae30-441e-9714-0befbea0fc3d", APIVersion:"v1", ResourceVersion:"703", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c5618fb0-df0c-4452-9946-42f4df31e5db became leader
I0812 20:44:27.915603       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c5618fb0-df0c-4452-9946-42f4df31e5db!

* 
* ==> storage-provisioner [cca015af045f] <==
* k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0001a61e0, 0x3b9aca00, 0x0, 0x1, 0xc0000950e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0001a61e0, 0x3b9aca00, 0xc0000950e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 80 [sync.Cond.Wait, 4 minutes]:
sync.runtime_notifyListWait(0xc0003d84d0, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0003d84c0)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000181aa0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc0001aef00, 0x18e5530, 0xc0003d9d40, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0001a6200)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0001a6200, 0x18b3d60, 0xc00023a420, 0x1, 0xc0000950e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0001a6200, 0x3b9aca00, 0x0, 0x1, 0xc0000950e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0001a6200, 0x3b9aca00, 0xc0000950e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 97 [sync.Cond.Wait, 4 minutes]:
sync.runtime_notifyListWait(0xc0003d8510, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0003d8500)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000181c20, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc0001aef00, 0x18e5530, 0xc0003d9d40, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0001a6220)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0001a6220, 0x18b3d60, 0xc000260150, 0x1, 0xc0000950e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0001a6220, 0x3b9aca00, 0x0, 0x1, 0xc0000950e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0001a6220, 0x3b9aca00, 0xc0000950e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 499 [runnable]:
k8s.io/apimachinery/pkg/util/wait.poller.func1.1(0xc000289b00, 0x77359400, 0x0, 0xc000289aa0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:588 +0x135
created by k8s.io/apimachinery/pkg/util/wait.poller.func1
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:571 +0x8c

